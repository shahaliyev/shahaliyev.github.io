{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## CSCI 4701: Training Deeper Networks: Batch Normalization and Residual Blocks"
      ],
      "metadata": {
        "id": "m4PEMI9MHhXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasing the number of layers in neural networks for learning more advanced functions is challenging due to issues like vanishing gradients. [VGGNet](https://arxiv.org/pdf/1409.1556) partially addressed this problem by using repetitive _blocks_ that stack multiple convolutional layers before downsampling with max-pooling. For instance, two consecutive `3x3` convolutional layers achieve the same receptive field as a single `5x5` convolution, while preserving a higher spatial resolution for the next layer. In simpler terms, repeating a smaller kernel allows the network to access the same input pixels while retaining more detail for subsequent processing. Larger kernels blur (downsample) the image more aggressively, which can lead to the loss of important details and force the network to reduce resolution earlier in the architecture and stop.\n",
        "\n",
        "Despite this breakthrough, VGGNet was still limited and showed diminishing returns beyond `19` layers (hence, `vgg19` architecture). Another architecture was introduced the same year with the paper titled [Going Deeper with Convolutions](https://arxiv.org/pdf/1409.4842). It was named **Inception** because of the [internet meme](https://knowyourmeme.com/memes/we-need-to-go-deeper) from the infamous _Inception_ movie. I am not joking. If you don't believe me, scroll down the paper for references section and check out the very first reference.\n",
        "\n",
        "Inception architecture, and its implementation, `GoogLeNet` model (a play on words: 1) was developed by Google researchers, and 2) pays homage to the LeNet architecture), significantly reduced parameter count and leveraged the advantages of the `1x1` convolution kernel (see the [Network in Network](https://arxiv.org/pdf/1312.4400) paper which also introduced `Global Average Pooling (GAP)` layer). Despite enabling deeper networks with far fewer parameters, Inception did not fully resolve the core training and convergence problems faced by very deep models.\n",
        "\n",
        "[Batch Normalization](https://arxiv.org/pdf/1502.03167) and [Residual Networks](https://arxiv.org/pdf/1512.03385) emerged as two major solutions for efficiently training neural networks as deep as `100` layers and more. We will now set up the data environment and go on discussing the core ideas and implementations of both papers."
      ],
      "metadata": {
        "id": "RG8ccN5fIOJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import random\n",
        "import string\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "32JyAp_Knhc3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########## DATA SETUP ##########\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\"\n",
        "response = requests.get(url)\n",
        "words = response.text.splitlines()\n",
        "random.shuffle(words)\n",
        "\n",
        "chars = list(string.ascii_lowercase)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "stoi['<START>'] = len(stoi)\n",
        "stoi['<END>'] = len(stoi)\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "\n",
        "BLOCK_SIZE = 3\n",
        "VOCAB_SIZE = len(stoi)\n",
        "EMBED_SIZE = 10\n",
        "LAYER_SIZE = 100\n",
        "\n",
        "len(words), BLOCK_SIZE, VOCAB_SIZE, words[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I70Lt1rUnlCr",
        "outputId": "cf2be4cf-c780-4e95-912a-8fc9b0d5b219"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32033, 3, 28, 'wafi')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A quick sidenote: it is encouraged to split the data into **training**, **validation** (also called _dev_), and **test** sets. When the dataset is not large, an `80/10/10` split is a reasonable ratio for allocation. For larger datasets (e.g. with one million images), it is fine to allocate `90%` or more of your data for training. The training set is used to update the model's _parameters_. The validation set is used for tuning _hyperparameters_ (e.g. testing different learning rates, regularization strengths, etc.). The test split should ideally be used only _once_ to report the final performance of the selected model (e.g. for inclusion in a research paper)."
      ],
      "metadata": {
        "id": "tf24AZ0xvC1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## DATA PREP ##########\n",
        "\n",
        "def get_ngrams(start=0, end=None):\n",
        "  X, Y = [], []\n",
        "  for word in words[start:end]:\n",
        "    context = ['<START>'] * BLOCK_SIZE\n",
        "    for ch in list(word) + ['<END>']:\n",
        "      X.append([stoi[c] for c in context])\n",
        "      Y.append(stoi[ch])\n",
        "      context = context[1:] + [ch]\n",
        "  return torch.tensor(X), torch.tensor(Y)\n",
        "\n",
        "def split_data(p=80):\n",
        "  train_end = int(p/100 * len(words))\n",
        "  remaining = len(words) - train_end\n",
        "  val_end = train_end + remaining // 2\n",
        "\n",
        "  X_train, Y_train = get_ngrams(end=train_end)\n",
        "  X_val, Y_val = get_ngrams(start=train_end, end=val_end)\n",
        "  X_test, Y_test = get_ngrams(start=val_end, end=len(words))\n",
        "\n",
        "  return {\n",
        "    'train': (X_train, Y_train),\n",
        "    'val':   (X_val, Y_val),\n",
        "    'test':  (X_test, Y_test),\n",
        "  }\n",
        "\n",
        "data = split_data()\n",
        "\n",
        "X_train, Y_train = data['train']\n",
        "X_val, Y_val = data['val']\n",
        "X_test, Y_test = data['test']\n",
        "\n",
        "len(X_train), len(X_val), len(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQo5Vpv2nzKQ",
        "outputId": "34d3d075-676b-4427-a3d4-848c8fffcca0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(182535, 22720, 22891)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Normalization"
      ],
      "metadata": {
        "id": "WY66NcMJtbBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch normalization normalizes the inputs within a mini-batch before passing them to the next layer. That is, for each input feature $x_i$, we subtract the batch mean and divide by the batch standard deviation. A small constant  $\\epsilon$ is commonly added for maintaining numerical stability (to avoid zero division):\n",
        "\n",
        "$$\n",
        "\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
        "$$\n",
        "\n",
        "This standardization gives $\\hat{x}_i$ a mean close to 0 and a standard deviation close to 1 over the batch. This may limit the model's capacity if left unchanged. Therefore, we introduce learnable parameters $\\gamma$ (scale) and $\\beta$ (shift) for flexibility:\n",
        "\n",
        "$$\n",
        "BN = \\gamma \\hat{x}_i + \\beta\n",
        "$$\n",
        "\n",
        "Batch normalization is typically applied after the affine transformation ($Wx + b$) and before the non-linearity (e.g., ReLU):\n",
        "\n",
        "$$\n",
        "act = \\phi(\\textrm{BN}(Wx))\n",
        "$$\n",
        "\n",
        "Pay attention that we omitted $b$ when using batch normalization. In practice, the bias $b$ becomes redundant, because the shifting role is already handled by $\\beta$. Recall that `PyTorch` has `bias=False` option  as well (e.g. in `nn.Conv2d()`).\n",
        "\n",
        "Batch normalization improves convergence in optimization and has regularization effect. The original paper by Ioffe and Szegedyattributes this to reducing _internal covariate shift_ â€” i.e. the shift in the distribution of layer inputs during training as parameters in earlier layers change. But this intuition is challenged. You can read more about that in [d2l book chapter](https://d2l.ai/chapter_convolutional-modern/batch-norm.html#discussion) dedicated to batch normalization."
      ],
      "metadata": {
        "id": "MqpPWUjYIsuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## PARAMETER SETUP ##########\n",
        "\n",
        "def get_params(batch_norm=True):\n",
        "  C = torch.randn((VOCAB_SIZE, EMBED_SIZE), requires_grad=True)\n",
        "\n",
        "  W1 = torch.randn((BLOCK_SIZE * EMBED_SIZE, LAYER_SIZE), requires_grad = True)\n",
        "  b1 = torch.zeros(LAYER_SIZE, requires_grad=True)\n",
        "\n",
        "  W2 = torch.randn((LAYER_SIZE, VOCAB_SIZE), requires_grad = True)\n",
        "  b2 = torch.zeros(VOCAB_SIZE, requires_grad=True)\n",
        "\n",
        "  params = {'C': C, 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
        "\n",
        "  if batch_norm:\n",
        "    gamma = torch.ones((1, LAYER_SIZE), requires_grad=True)\n",
        "    beta = torch.zeros((1, LAYER_SIZE), requires_grad=True)\n",
        "    params['gamma'] = gamma\n",
        "    params['beta'] = beta\n",
        "    # we can add additional code for omitting b1 in case of using beta (BN bias)\n",
        "\n",
        "  return params"
      ],
      "metadata": {
        "id": "IV5z4fBsiRta"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### running_stats\n",
        "\n",
        "In `PyTorch` we use `model.eval()` during inference to switch the model into evaluation mode. This is important because layers like dropout and batch normalization behave differently during training and evaluation.\n",
        "\n",
        "During inference, normalization should be done using statistics over the whole dataset instead of mini-batches. Without `bn_stats` in the code below, the model would normalize using the current batch's mean and standard deviation, leading to inconsistent results depending on the batch.\n",
        "\n",
        "The implemented `PyTorch` layers like [nn.BatchNorm1d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) automatically calculate **running statistics** during training. These statistics include a running mean and a running variance for each feature channel, which are stored as non-learnable buffers inside the `BatchNorm` layer.\n",
        "\n",
        "$$\n",
        "\\mu_{\\text{running}} = \\alpha \\, \\mu_{\\text{batch}} + (1 - \\alpha) \\, \\mu_{\\text{running}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma^2_{\\text{running}} = \\alpha \\, \\sigma^2_{\\text{batch}} + (1 - \\alpha) \\, \\sigma^2_{\\text{running}}\n",
        "$$\n",
        "\n",
        "In `BatchNorm`, $\\alpha$ is defined as `momentum` which is a misnomer and has nothing to do with the momentum we had previously learned for optimization. Its values controls how quickly the `running_stats` adapt. If momentum is high, the running statistics update quickly based on new batches which can make them unstable and noisy if batches vary a lot. If it is low (by default it is set to `0.1`, but you may want to reduce it further depending on circumstances), the updates are smoother and slower, averaging batch statistics over time.\n",
        "\n",
        "During evaluation `BatchNorm` uses the stored running mean and variance for normalization. This ensures deterministic behavior, regardless of the input batch. These buffers are automatically updated and used unless you disable tracking by setting `track_running_stats=False`.\n",
        "\n",
        "A manual implementation of `running_stats` is demonstrated in [Andrej Karpathy's video](https://www.youtube.com/watch?v=P6sfmUTpUmc) as well. In this notebook, we will only implemented the simpler `bn_stats`."
      ],
      "metadata": {
        "id": "01Vyj6wTcpqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## FORWARD PASS ##########\n",
        "\n",
        "@torch.no_grad() # applies \"with torch.no_grad()\" to the whole function\n",
        "def get_bn_stats(X_train, params):\n",
        "  emb = params['C'][X_train]\n",
        "  out = emb.view(emb.shape[0], -1) @ params['W1'] + params['b1']\n",
        "  mean, std = out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5\n",
        "  return mean, std\n",
        "\n",
        "def forward(X, params, batch_norm=False, bn_stats=None):\n",
        "  emb = params['C'][X]\n",
        "  out = emb.view(emb.shape[0], -1) @ params['W1'] + params['b1']\n",
        "\n",
        "  if batch_norm:\n",
        "    mean, std = bn_stats if bn_stats else (out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5)\n",
        "    out = (out - mean) / std\n",
        "    out = params['gamma'] * out + params['beta']\n",
        "\n",
        "  act = torch.tanh(out)\n",
        "  logits = act @ params['W2'] + params['b2']\n",
        "  return logits"
      ],
      "metadata": {
        "id": "zVGx7xEWnaB3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########## TRAINING & INFERENCE ##########\n",
        "\n",
        "def train(X, Y, params, num_epochs=100, lr=0.1, batch_size=None, batch_norm=False):\n",
        "  for epoch in range(1, num_epochs+1):\n",
        "    if batch_size:\n",
        "      idx = torch.randint(0, X.size(0), (batch_size,))\n",
        "      batch_X, batch_Y = X[idx], Y[idx]\n",
        "    else:\n",
        "      batch_X, batch_Y = X, Y\n",
        "\n",
        "    logits = forward(batch_X, params, batch_norm)\n",
        "    loss = F.cross_entropy(logits, batch_Y)\n",
        "\n",
        "    for p in params.values():\n",
        "      if p.grad is not None:\n",
        "        p.grad.zero_()\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for p in params.values():\n",
        "        p.data -= lr * p.grad\n",
        "\n",
        "    if epoch % (1000 if batch_size else 10) == 0:\n",
        "      print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(X, Y, params, batch_norm=False, bn_stats=None):\n",
        "  logits = forward(X, params, batch_norm, bn_stats)\n",
        "  loss = F.cross_entropy(logits, Y)\n",
        "  print(f\"Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "oksSQ1mzr3BY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########## TEST ##########\n",
        "\n",
        "epochs = 10_000\n",
        "lr = 0.01\n",
        "batch_size = 32\n",
        "batch_norm = True\n",
        "init = True"
      ],
      "metadata": {
        "id": "f13rFKzPvTOL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = get_params(batch_norm=batch_norm)\n",
        "\n",
        "# xavier for tanh, kaiming for relu\n",
        "if init:\n",
        "  nn.init.xavier_uniform_(params['W1'])\n",
        "  nn.init.xavier_uniform_(params['W2'])"
      ],
      "metadata": {
        "id": "LmhkzIjF0uQW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(X_train, Y_train, params, num_epochs=epochs, lr=lr, batch_size=batch_size, batch_norm=batch_norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmyetGr50taS",
        "outputId": "53d60d59-6447-411c-929f-cefdc28646fb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1000, Loss: 2.7862\n",
            "Epoch 2000, Loss: 2.5897\n",
            "Epoch 3000, Loss: 2.3340\n",
            "Epoch 4000, Loss: 2.4985\n",
            "Epoch 5000, Loss: 2.7501\n",
            "Epoch 6000, Loss: 2.2715\n",
            "Epoch 7000, Loss: 2.6008\n",
            "Epoch 8000, Loss: 2.0696\n",
            "Epoch 9000, Loss: 2.3910\n",
            "Epoch 10000, Loss: 2.4777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bn_stats = get_bn_stats(X_train, params) if batch_norm else None\n",
        "\n",
        "print('Train and Validation losses:')\n",
        "evaluate(X_train, Y_train, params, batch_norm=batch_norm, bn_stats=bn_stats)\n",
        "evaluate(X_val, Y_val, params, batch_norm=batch_norm, bn_stats=bn_stats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Rs3aXCXzS8Z",
        "outputId": "f191600e-cc6d-4254-a2cc-95356f727a89"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train and Validation losses:\n",
            "Loss: 2.3337\n",
            "Loss: 2.3345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########## SAMPLING ##########\n",
        "\n",
        "# minor changes to what we had previously for adapting to new code\n",
        "def sample(params, n=10, batch_norm=False, bn_stats=None):\n",
        "  names = []\n",
        "  for _ in range(n):\n",
        "    context = ['<START>'] * BLOCK_SIZE\n",
        "    name = ''\n",
        "    while True:\n",
        "      X = torch.tensor([[stoi[c] for c in context]])\n",
        "      logits = forward(X, params, batch_norm, bn_stats)\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      id = torch.multinomial(probs, num_samples=1).item()\n",
        "      char = itos[id]\n",
        "      if char == '<END>':\n",
        "        break\n",
        "      name += char\n",
        "      context = context[1:] + [char]\n",
        "    names.append(name)\n",
        "  return names"
      ],
      "metadata": {
        "id": "arZjJW1UtFu5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample(params, batch_norm=batch_norm, bn_stats=bn_stats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z7iczV2wEVU",
        "outputId": "0496d6b7-63cb-45ac-c621-6d5dc720c564"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['khuc',\n",
              " 'boka',\n",
              " 'lyq',\n",
              " 'rasyanrith',\n",
              " 'onna',\n",
              " 'helia',\n",
              " 'brhaylanio',\n",
              " 'boleiklak',\n",
              " 'ekbnqron',\n",
              " 'aren']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer Normalization\n",
        "\n",
        "A rule of thumb is that batch sizes between `50-100` generally work well for batch normalization: the batch is large enough to return reliable statistics but not so large that it causes memory issues or slows down training unnecessarily. Batch size of `32` is usually the lower bound where batch normalization still provides relatively stable estimates. Batch size of `128` is also effective if the hardware allows, and can produce even smoother estimates. Beyond that the benefit often diminishes.\n",
        "\n",
        "If the batch size is very small due to memory limitations, batch normalization may lose its effectiveness. In such cases, it's better to consider alternatives like [Layer Normalization](https://arxiv.org/abs/1607.06450) which do not depend on the batch dimension.\n",
        "\n",
        "Layer normalization normalizes across features for each individual sample, not across the batch and works well for _transformers_ where batch sizes may be small or variable. Basically, batch normalization depends on the batch, but layer normalization does not.\n",
        "\n",
        "Furthermore, in fully connected layers, each feature is just a single number per sample, so batch normalization computes the mean and variance across the batch for each feature. Fully connected layers don't have spatial structure, so there's nothing to average across except the batch. In convolutional layers, each feature channel height and width and is a 2D map (hence, `nn.BatchNorm2d`), so batch normalization uses not just the batch dimension, but also all the spatial positions to compute statistics. This gives more stable estimates because there are more values per channel."
      ],
      "metadata": {
        "id": "Kg5b8kmixyBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Residual Block\n",
        "\n",
        "**Residual Network (ResNet)** consists of repeated _residual blocks_, in the style of the VGGNet architecture. Each residual block consists of a _residual (skip/shortcut) connection_ . We will first see what it does and then will attempt to understand the reasoning behind this simple breakthrough idea."
      ],
      "metadata": {
        "id": "JcXeePmP8aFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation\n",
        "\n",
        "![Residual Block](https://d2l.ai/_images/residual-block.svg)\n",
        "\n",
        "_Figure 8.6.2_ of [Dive into Deep Learning (Chapter 8)](https://d2l.ai/chapter_convolutional-modern/resnet.html) by [d2l.ai](https://d2l.ai/) authors and contributors. Licensed under [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)"
      ],
      "metadata": {
        "id": "gf9uK1ZjSt5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hence, the idea of the residual connection is very simple. Before the second activation function, we add the previous input to the affine transformation. You can imagine the simplified code as below:"
      ],
      "metadata": {
        "id": "yL6kUodD1DQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def residual_block(X):\n",
        "  act = torch.relu(X @ params['W1'] + params['b1'])\n",
        "  out = act @ params['W2'] + params['b2']\n",
        "  return torch.relu(out + X)"
      ],
      "metadata": {
        "id": "z-36sEZT7lDf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, If we attempt to directly run the code above, we will see a shape mismatch, as our final layer returns a matrix of dimension `VOCAB_SIZE` which is not equal to the input dimension `BLOCK_SIZE * EMBED_SIZE`."
      ],
      "metadata": {
        "id": "JBVFlAE77olk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:** Modifying the `forward` function by adding a residual connection."
      ],
      "metadata": {
        "id": "oGFunjp_7Qm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(X, params, batch_norm=False, bn_stats=None, residual=True):\n",
        "  emb = params['C'][X]\n",
        "  out = emb.view(emb.shape[0], -1) @ params['W1'] + params['b1']\n",
        "\n",
        "  if batch_norm:\n",
        "    mean, std = bn_stats if bn_stats else (out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5)\n",
        "    out = (out - mean) / std\n",
        "    out = params['gamma'] * out + params['beta']\n",
        "\n",
        "  act = torch.tanh(out + emb) if residual else torch.tanh(out)\n",
        "  logits = act @ params['W2'] + params['b2']\n",
        "  return logits"
      ],
      "metadata": {
        "id": "b5XheW_H6l42"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = params['C'][X_train].view(X_train.shape[0], -1)\n",
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu3FPAl63PZw",
        "outputId": "93e1aa7c-c544-4f88-9d40-8a3d7b680fb0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([182535, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What to do? For demonstration purposes we will have to add another layer.\n",
        "\n",
        "**Exercise (Advanced)**: Train a three layer model with batch normalization and residual connections."
      ],
      "metadata": {
        "id": "Sp45tKHS08E4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_params(batch_norm=True):\n",
        "  C = torch.randn((VOCAB_SIZE, EMBED_SIZE), requires_grad=True)\n",
        "\n",
        "  in_features = BLOCK_SIZE * EMBED_SIZE\n",
        "\n",
        "  W1 = torch.randn((in_features, LAYER_SIZE), requires_grad = True)\n",
        "  b1 = torch.zeros(LAYER_SIZE, requires_grad=True)\n",
        "\n",
        "  W2 = torch.randn((LAYER_SIZE, in_features), requires_grad = True)\n",
        "  b2 = torch.zeros(in_features, requires_grad=True)\n",
        "\n",
        "  W3 = torch.randn((in_features, VOCAB_SIZE), requires_grad = True)\n",
        "  b3 = torch.zeros(VOCAB_SIZE, requires_grad=True)\n",
        "\n",
        "  params = {'C': C, 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3': W3, 'b3': b3}\n",
        "\n",
        "  if batch_norm:\n",
        "    gamma = torch.ones((1, LAYER_SIZE), requires_grad=True)\n",
        "    beta = torch.zeros((1, LAYER_SIZE), requires_grad=True)\n",
        "    params['gamma'] = gamma\n",
        "    params['beta'] = beta\n",
        "\n",
        "  return params"
      ],
      "metadata": {
        "id": "gOIIfjok5vy7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(X, params, batch_norm=False, bn_stats=None, residual=True):\n",
        "  emb = params['C'][X].view(X.shape[0], -1)\n",
        "  out = emb @ params['W1'] + params['b1']\n",
        "\n",
        "  if batch_norm:\n",
        "    mean, std = bn_stats if bn_stats else (out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5)\n",
        "    out = (out - mean) / std\n",
        "    out = params['gamma'] * out + params['beta']\n",
        "\n",
        "  act = torch.relu(out)\n",
        "  out2 = act @ params['W2'] + params['b2']\n",
        "\n",
        "  if residual:\n",
        "    out2 = out2 + emb\n",
        "\n",
        "  logits = torch.tanh(out2) @ params['W3'] + params['b3']\n",
        "  return logits"
      ],
      "metadata": {
        "id": "2f4bfTpm8bDK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = get_params()\n",
        "params.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4vzyiU2AA-q",
        "outputId": "a1484891-01e9-40f7-976d-81c161cf0b2b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['C', 'W1', 'b1', 'W2', 'b2', 'W3', 'b3', 'gamma', 'beta'])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we are using relu in intermediate layer\n",
        "if init:\n",
        "  nn.init.kaiming_uniform_(params['W1'])\n",
        "  nn.init.kaiming_uniform_(params['W2']);"
      ],
      "metadata": {
        "id": "X4tdVn9uBi0K"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(X_train, Y_train, params, num_epochs=epochs, lr=lr, batch_size=batch_size, batch_norm=batch_norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zzsbi0pN_uT8",
        "outputId": "ed290c15-da8f-4909-aca3-bfa0fc354dc8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1000, Loss: 2.5227\n",
            "Epoch 2000, Loss: 2.9970\n",
            "Epoch 3000, Loss: 2.5845\n",
            "Epoch 4000, Loss: 2.3321\n",
            "Epoch 5000, Loss: 2.2630\n",
            "Epoch 6000, Loss: 2.5062\n",
            "Epoch 7000, Loss: 2.8853\n",
            "Epoch 8000, Loss: 2.3080\n",
            "Epoch 9000, Loss: 2.7023\n",
            "Epoch 10000, Loss: 2.8854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bn_stats = get_bn_stats(X_train, params) if batch_norm else None\n",
        "\n",
        "print('Train and Validation losses:')\n",
        "evaluate(X_train, Y_train, params, batch_norm=batch_norm, bn_stats=bn_stats)\n",
        "evaluate(X_val, Y_val, params, batch_norm=batch_norm, bn_stats=bn_stats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtqXqm5I_6Gn",
        "outputId": "df853bfd-0a99-4dfc-de1c-2d15959a4f0c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train and Validation losses:\n",
            "Loss: 2.3690\n",
            "Loss: 2.3698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reasoning\n",
        "\n",
        "As our model is implementing a single residual block, we don't see any performance improvement. However, similar to batch normalization, the advantages will be obvious in case of 50 layers or more, with repeated residual blocks. But why adding input of the layer to the second affine transformation boosts training?\n",
        "\n",
        "Let's take any deep learning model. The types of functions this model can learn depend on its design (e.g. number of layers, activation functions, etc). All these possible functions we can denote as class $\\mathcal{F}$. If we cannot learn a perfect function for our data, which is usually the case, we can at least try to appoximate this function as closely as possible by minimizing a loss. We may assume that a more powerful model can learn more types of functions and show better performance. But that's not always the case. To achieve a better performance than a simpler model, our model must be capable of learning not only more functions but also all the functions the simpler model can learn. Simply, the possible function class of the more powerful model should be a superclass of the simpler model's function class $\\mathcal{F} \\subseteq \\mathcal{F}'$. If the ${F}'$ isn't an expanded version of {F}$, the new model might actually learn a function that is farther from the truth, and even show worse performance.\n",
        "\n",
        "Refer to the figure above, where our residual output is $f(x) = g(x) + x$. One advantage of residual blocks is their regularization effect. What if some activation nodes in our network are unnecessary and increase complexity or learn bad representations? Instead of learning weights and biases, our residual block can now learn an identity function $f(x) = x$ by simply setting that nodes parameters to zero. As a result, our inputs will propagate faster while ensuring that the learned functions are within the biggest function domain. Residual blocks not only act as a regularizer, but also, unlike, say, _dropout_ which stops input from propagating, allow the network to learn more functions by helping inputs to \"jump over\" (skip) the nodes. And it is very important that the function classes of the model with residual blocks is a superset of the same model without such blocks. Finally, along the way, it deals with the vanishing gradient problem by simply increasing the output of each layer. To sum up, residual connection allows the model to learn more complex functions, while allowing it to easily learn simpler ones, which tackles the vanishing gradient problem and has a regularizing effect."
      ],
      "metadata": {
        "id": "nKy44kFjE9yS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Residual Network for NLP in PyTorch\n",
        "\n",
        "Originally, the complete Residual Network was developed for image classification tasks, winning _ImageNet_ competition. Each of its residual block consisted of two `3x3` convolutions (inspired  by _VGGNet_), both integrating batch normalization, followed by a skip connection. Even though, ResNet model relies on convolutional layer, the concept of residual connections has been adapted for NLP models as well. The infamous **Transformer** model, introduced in the paper titled [Attention is All You Need](https://arxiv.org/pdf/1706.03762) incorporates residual connections heavily in its design, which is very similar to ResNet."
      ],
      "metadata": {
        "id": "_c9Au8a_VY9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(VOCAB_SIZE, EMBED_SIZE)\n",
        "    self.fc1 = nn.Linear(in_features=EMBED_SIZE, out_features=LAYER_SIZE, bias=False)\n",
        "    self.fc2 = nn.Linear(in_features=LAYER_SIZE, out_features=EMBED_SIZE, bias=False)\n",
        "    self.fc3 = nn.Linear(in_features=EMBED_SIZE, out_features=VOCAB_SIZE, bias=True)\n",
        "    self.bn1 = nn.LazyBatchNorm1d()\n",
        "    self.bn2 = nn.LazyBatchNorm1d()\n",
        "    nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n",
        "\n",
        "  # nn.LazyBatchNorm1d in 3D input expects shape (batch, channels, length) = (B, C, T)\n",
        "  # it normalizes across the batch and time (token, block) dimensions for each channel, independently\n",
        "  # we need to move that dimension to the middle (axis 1) with transpose(1, 2)\n",
        "  def forward(self, X):\n",
        "    emb = self.emb(X)                     # (BATCH_SIZE, BLOCK_SIZE, EMBED_SIZE)\n",
        "    out = self.fc1(emb).transpose(1, 2)   # (BATCH_SIZE, LAYER_SIZE, BLOCK_SIZE) for BatchNorm1d\n",
        "    out = self.bn1(out).transpose(1, 2)   # back to our dimensions\n",
        "    act = F.relu(out)\n",
        "    out = self.fc2(act).transpose(1, 2)\n",
        "    out = self.bn2(out).transpose(1, 2)\n",
        "    out += emb                            # shortcut connection\n",
        "    logits = self.fc3(out)                # (BATCH_SIZE, BLOCK_SIZE, VOCAB_SIZE)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "XOxrmDUIH78g"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResidualBlock()\n",
        "cel = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j90NDExrnD4r",
        "outputId": "c99e34e9-5405-4e2a-9c84-12586957447b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResidualBlock(\n",
              "  (emb): Embedding(28, 10)\n",
              "  (fc1): Linear(in_features=10, out_features=100, bias=False)\n",
              "  (fc2): Linear(in_features=100, out_features=10, bias=False)\n",
              "  (fc3): Linear(in_features=10, out_features=28, bias=True)\n",
              "  (bn1): LazyBatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (bn2): LazyBatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10_000\n",
        "batch_size = 32\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "  model.train()\n",
        "  idx = torch.randint(0, X_train.size(0), (batch_size,))\n",
        "  batch_X, batch_Y = X_train[idx], Y_train[idx]\n",
        "  optimizer.zero_grad()\n",
        "  logits = model(batch_X)     # (BATCH_SIZE, BLOCK_SIZE, VOCAB_SIZE)\n",
        "  logits = logits[:, -1, :]   # (BATCH_SIZE, VOCAB_SIZE)\n",
        "  loss = cel(logits, batch_Y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if epoch % 1000 == 0 or epoch == 1:\n",
        "    print(f'Epoch {epoch}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUWlKB-Jth--",
        "outputId": "9b38606e-3c3a-4310-a3d9-6a746663660d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 3.6320574283599854\n",
            "Epoch 1000, Loss: 2.374105930328369\n",
            "Epoch 2000, Loss: 2.6409666538238525\n",
            "Epoch 3000, Loss: 2.6358656883239746\n",
            "Epoch 4000, Loss: 2.36672043800354\n",
            "Epoch 5000, Loss: 2.696502208709717\n",
            "Epoch 6000, Loss: 2.4992451667785645\n",
            "Epoch 7000, Loss: 2.413964033126831\n",
            "Epoch 8000, Loss: 2.83028507232666\n",
            "Epoch 9000, Loss: 2.3721745014190674\n",
            "Epoch 10000, Loss: 2.6832263469696045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  logits_train = model(X_train)[:, -1, :]\n",
        "  logits_val   = model(X_val)[:, -1, :]\n",
        "\n",
        "  full_loss_train = cel(logits_train, Y_train)\n",
        "  full_loss_val   = cel(logits_val, Y_val)\n",
        "\n",
        "  print(f'Train loss: {full_loss_train.item()}')\n",
        "  print(f'Validation loss: {full_loss_val.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpJRfbTO_ZU-",
        "outputId": "7f18bc9e-e2c1-4a9e-9ccf-ba00cc3a889c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 2.4901065826416016\n",
            "Validation loss: 2.4812421798706055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# modifying code to suit our needs\n",
        "def sample(model, n=10, block_size=3):\n",
        "  model.eval()\n",
        "  names = []\n",
        "  for _ in range(n):\n",
        "    context = ['<START>'] * block_size\n",
        "    name = ''\n",
        "    while True:\n",
        "      idx = [stoi[c] for c in context]\n",
        "      X = torch.tensor([idx], dtype=torch.long)\n",
        "      with torch.no_grad():\n",
        "        logits = model(X)[0, -1] # VOCAB_SIZE\n",
        "      probs = F.softmax(logits, dim=0)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1).item()\n",
        "      char = itos[idx_next]\n",
        "      if char == '<END>':\n",
        "        break\n",
        "      name += char\n",
        "      context = context[1:] + [char]\n",
        "    names.append(name)\n",
        "  return names"
      ],
      "metadata": {
        "id": "RQIcPvRPtiYz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2U1OxFuG36U_",
        "outputId": "d2d4f8f6-3761-4da2-8a30-154613373495"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['kelifo',\n",
              " 'ja',\n",
              " 'tha',\n",
              " 'elarhncasoria',\n",
              " 'ka',\n",
              " 'voratte',\n",
              " 'eniysh',\n",
              " 'th',\n",
              " 'kelld',\n",
              " 'edm']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0OtQtTtcMO_f"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}