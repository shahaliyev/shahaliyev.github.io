{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CSCI 4701: Python Code for Neural Network N-Gram Model\n",
        "\n",
        "Highly based on Andrej Karpathy's [makemore](https://github.com/karpathy/makemore). See the [original video](https://www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=3)."
      ],
      "metadata": {
        "id": "3XC15iNsA03J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wV_lJhFUAUEw"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\"\n",
        "response = requests.get(url)\n",
        "data = response.text\n",
        "words = data.splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Length:', len(words))\n",
        "print(words[:5] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Gwq_002BXPh",
        "outputId": "95145c73-de46-484c-c811-e5259a03b674"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length: 32033\n",
            "['emma', 'olivia', 'ava', 'isabella', 'sophia']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This data provides information about names. For example, by having five examples `['emma', 'olivia', 'ava', 'isabella', 'sophia']` we may conclude that the probability of `'a'` being the last letter is `1.0` after which the word will certainly end, or that the letter `'o'` is more likely to be at the beginning of the name. Our goal will be to to predict the most probable next character. A common technique is to take track of bigrams of characters (there can be N-grams of words, etc., which have both advantages and disadvantages)."
      ],
      "metadata": {
        "id": "VEfTWjkqCgub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bigram Model"
      ],
      "metadata": {
        "id": "FqQsr_sr_fVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bigrams(n):\n",
        "  bigrams = {}\n",
        "  for w in words[:n]:\n",
        "    w = ['<START>'] + list(w) + ['<END>']\n",
        "    for ch1, ch2 in zip(w, w[1:]):\n",
        "      b = (ch1, ch2)\n",
        "      bigrams[b] = bigrams.get(b, 0) + 1\n",
        "  return bigrams"
      ],
      "metadata": {
        "id": "U8m_hV9pBjiF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change `n` up to 5 and take note of the bigram counts."
      ],
      "metadata": {
        "id": "QeXyRc5IP23I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_bigrams(n=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THhUvYk4ulB2",
        "outputId": "203215f0-f6ab-4c1c-89dc-dddf40119140"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('<START>', 'e'): 1,\n",
              " ('e', 'm'): 1,\n",
              " ('m', 'm'): 1,\n",
              " ('m', 'a'): 1,\n",
              " ('a', '<END>'): 2,\n",
              " ('<START>', 'o'): 1,\n",
              " ('o', 'l'): 1,\n",
              " ('l', 'i'): 1,\n",
              " ('i', 'v'): 1,\n",
              " ('v', 'i'): 1,\n",
              " ('i', 'a'): 1}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "bigrams = get_bigrams(len(words))\n",
        "Counter(bigrams).most_common()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3VxwHcOQc_q",
        "outputId": "1610be0c-a4f2-475d-b256-06894ec79da4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('n', '<END>'), 6763),\n",
              " (('a', '<END>'), 6640),\n",
              " (('a', 'n'), 5438),\n",
              " (('<START>', 'a'), 4410),\n",
              " (('e', '<END>'), 3983),\n",
              " (('a', 'r'), 3264),\n",
              " (('e', 'l'), 3248),\n",
              " (('r', 'i'), 3033),\n",
              " (('n', 'a'), 2977),\n",
              " (('<START>', 'k'), 2963),\n",
              " (('l', 'e'), 2921),\n",
              " (('e', 'n'), 2675),\n",
              " (('l', 'a'), 2623),\n",
              " (('m', 'a'), 2590),\n",
              " (('<START>', 'm'), 2538),\n",
              " (('a', 'l'), 2528),\n",
              " (('i', '<END>'), 2489),\n",
              " (('l', 'i'), 2480),\n",
              " (('i', 'a'), 2445),\n",
              " (('<START>', 'j'), 2422),\n",
              " (('o', 'n'), 2411),\n",
              " (('h', '<END>'), 2409),\n",
              " (('r', 'a'), 2356),\n",
              " (('a', 'h'), 2332),\n",
              " (('h', 'a'), 2244),\n",
              " (('y', 'a'), 2143),\n",
              " (('i', 'n'), 2126),\n",
              " (('<START>', 's'), 2055),\n",
              " (('a', 'y'), 2050),\n",
              " (('y', '<END>'), 2007),\n",
              " (('e', 'r'), 1958),\n",
              " (('n', 'n'), 1906),\n",
              " (('y', 'n'), 1826),\n",
              " (('k', 'a'), 1731),\n",
              " (('n', 'i'), 1725),\n",
              " (('r', 'e'), 1697),\n",
              " (('<START>', 'd'), 1690),\n",
              " (('i', 'e'), 1653),\n",
              " (('a', 'i'), 1650),\n",
              " (('<START>', 'r'), 1639),\n",
              " (('a', 'm'), 1634),\n",
              " (('l', 'y'), 1588),\n",
              " (('<START>', 'l'), 1572),\n",
              " (('<START>', 'c'), 1542),\n",
              " (('<START>', 'e'), 1531),\n",
              " (('j', 'a'), 1473),\n",
              " (('r', '<END>'), 1377),\n",
              " (('n', 'e'), 1359),\n",
              " (('l', 'l'), 1345),\n",
              " (('i', 'l'), 1345),\n",
              " (('i', 's'), 1316),\n",
              " (('l', '<END>'), 1314),\n",
              " (('<START>', 't'), 1308),\n",
              " (('<START>', 'b'), 1306),\n",
              " (('d', 'a'), 1303),\n",
              " (('s', 'h'), 1285),\n",
              " (('d', 'e'), 1283),\n",
              " (('e', 'e'), 1271),\n",
              " (('m', 'i'), 1256),\n",
              " (('s', 'a'), 1201),\n",
              " (('s', '<END>'), 1169),\n",
              " (('<START>', 'n'), 1146),\n",
              " (('a', 's'), 1118),\n",
              " (('y', 'l'), 1104),\n",
              " (('e', 'y'), 1070),\n",
              " (('o', 'r'), 1059),\n",
              " (('a', 'd'), 1042),\n",
              " (('t', 'a'), 1027),\n",
              " (('<START>', 'z'), 929),\n",
              " (('v', 'i'), 911),\n",
              " (('k', 'e'), 895),\n",
              " (('s', 'e'), 884),\n",
              " (('<START>', 'h'), 874),\n",
              " (('r', 'o'), 869),\n",
              " (('e', 's'), 861),\n",
              " (('z', 'a'), 860),\n",
              " (('o', '<END>'), 855),\n",
              " (('i', 'r'), 849),\n",
              " (('b', 'r'), 842),\n",
              " (('a', 'v'), 834),\n",
              " (('m', 'e'), 818),\n",
              " (('e', 'i'), 818),\n",
              " (('c', 'a'), 815),\n",
              " (('i', 'y'), 779),\n",
              " (('r', 'y'), 773),\n",
              " (('e', 'm'), 769),\n",
              " (('s', 't'), 765),\n",
              " (('h', 'i'), 729),\n",
              " (('t', 'e'), 716),\n",
              " (('n', 'd'), 704),\n",
              " (('l', 'o'), 692),\n",
              " (('a', 'e'), 692),\n",
              " (('a', 't'), 687),\n",
              " (('s', 'i'), 684),\n",
              " (('e', 'a'), 679),\n",
              " (('d', 'i'), 674),\n",
              " (('h', 'e'), 674),\n",
              " (('<START>', 'g'), 669),\n",
              " (('t', 'o'), 667),\n",
              " (('c', 'h'), 664),\n",
              " (('b', 'e'), 655),\n",
              " (('t', 'h'), 647),\n",
              " (('v', 'a'), 642),\n",
              " (('o', 'l'), 619),\n",
              " (('<START>', 'i'), 591),\n",
              " (('i', 'o'), 588),\n",
              " (('e', 't'), 580),\n",
              " (('v', 'e'), 568),\n",
              " (('a', 'k'), 568),\n",
              " (('a', 'a'), 556),\n",
              " (('c', 'e'), 551),\n",
              " (('a', 'b'), 541),\n",
              " (('i', 't'), 541),\n",
              " (('<START>', 'y'), 535),\n",
              " (('t', 'i'), 532),\n",
              " (('s', 'o'), 531),\n",
              " (('m', '<END>'), 516),\n",
              " (('d', '<END>'), 516),\n",
              " (('<START>', 'p'), 515),\n",
              " (('i', 'c'), 509),\n",
              " (('k', 'i'), 509),\n",
              " (('o', 's'), 504),\n",
              " (('n', 'o'), 496),\n",
              " (('t', '<END>'), 483),\n",
              " (('j', 'o'), 479),\n",
              " (('u', 's'), 474),\n",
              " (('a', 'c'), 470),\n",
              " (('n', 'y'), 465),\n",
              " (('e', 'v'), 463),\n",
              " (('s', 's'), 461),\n",
              " (('m', 'o'), 452),\n",
              " (('i', 'k'), 445),\n",
              " (('n', 't'), 443),\n",
              " (('i', 'd'), 440),\n",
              " (('j', 'e'), 440),\n",
              " (('a', 'z'), 435),\n",
              " (('i', 'g'), 428),\n",
              " (('i', 'm'), 427),\n",
              " (('r', 'r'), 425),\n",
              " (('d', 'r'), 424),\n",
              " (('<START>', 'f'), 417),\n",
              " (('u', 'r'), 414),\n",
              " (('r', 'l'), 413),\n",
              " (('y', 's'), 401),\n",
              " (('<START>', 'o'), 394),\n",
              " (('e', 'd'), 384),\n",
              " (('a', 'u'), 381),\n",
              " (('c', 'o'), 380),\n",
              " (('k', 'y'), 379),\n",
              " (('d', 'o'), 378),\n",
              " (('<START>', 'v'), 376),\n",
              " (('t', 't'), 374),\n",
              " (('z', 'e'), 373),\n",
              " (('z', 'i'), 364),\n",
              " (('k', '<END>'), 363),\n",
              " (('g', 'h'), 360),\n",
              " (('t', 'r'), 352),\n",
              " (('k', 'o'), 344),\n",
              " (('t', 'y'), 341),\n",
              " (('g', 'e'), 334),\n",
              " (('g', 'a'), 330),\n",
              " (('l', 'u'), 324),\n",
              " (('b', 'a'), 321),\n",
              " (('d', 'y'), 317),\n",
              " (('c', 'k'), 316),\n",
              " (('<START>', 'w'), 307),\n",
              " (('k', 'h'), 307),\n",
              " (('u', 'l'), 301),\n",
              " (('y', 'e'), 301),\n",
              " (('y', 'r'), 291),\n",
              " (('m', 'y'), 287),\n",
              " (('h', 'o'), 287),\n",
              " (('w', 'a'), 280),\n",
              " (('s', 'l'), 279),\n",
              " (('n', 's'), 278),\n",
              " (('i', 'z'), 277),\n",
              " (('u', 'n'), 275),\n",
              " (('o', 'u'), 275),\n",
              " (('n', 'g'), 273),\n",
              " (('y', 'd'), 272),\n",
              " (('c', 'i'), 271),\n",
              " (('y', 'o'), 271),\n",
              " (('i', 'v'), 269),\n",
              " (('e', 'o'), 269),\n",
              " (('o', 'm'), 261),\n",
              " (('r', 'u'), 252),\n",
              " (('f', 'a'), 242),\n",
              " (('b', 'i'), 217),\n",
              " (('s', 'y'), 215),\n",
              " (('n', 'c'), 213),\n",
              " (('h', 'y'), 213),\n",
              " (('p', 'a'), 209),\n",
              " (('r', 't'), 208),\n",
              " (('q', 'u'), 206),\n",
              " (('p', 'h'), 204),\n",
              " (('h', 'r'), 204),\n",
              " (('j', 'u'), 202),\n",
              " (('g', 'r'), 201),\n",
              " (('p', 'e'), 197),\n",
              " (('n', 'l'), 195),\n",
              " (('y', 'i'), 192),\n",
              " (('g', 'i'), 190),\n",
              " (('o', 'd'), 190),\n",
              " (('r', 's'), 190),\n",
              " (('r', 'd'), 187),\n",
              " (('h', 'l'), 185),\n",
              " (('s', 'u'), 185),\n",
              " (('a', 'x'), 182),\n",
              " (('e', 'z'), 181),\n",
              " (('e', 'k'), 178),\n",
              " (('o', 'v'), 176),\n",
              " (('a', 'j'), 175),\n",
              " (('o', 'h'), 171),\n",
              " (('u', 'e'), 169),\n",
              " (('m', 'm'), 168),\n",
              " (('a', 'g'), 168),\n",
              " (('h', 'u'), 166),\n",
              " (('x', '<END>'), 164),\n",
              " (('u', 'a'), 163),\n",
              " (('r', 'm'), 162),\n",
              " (('a', 'w'), 161),\n",
              " (('f', 'i'), 160),\n",
              " (('z', '<END>'), 160),\n",
              " (('u', '<END>'), 155),\n",
              " (('u', 'm'), 154),\n",
              " (('e', 'c'), 153),\n",
              " (('v', 'o'), 153),\n",
              " (('e', 'h'), 152),\n",
              " (('p', 'r'), 151),\n",
              " (('d', 'd'), 149),\n",
              " (('o', 'a'), 149),\n",
              " (('w', 'e'), 149),\n",
              " (('w', 'i'), 148),\n",
              " (('y', 'm'), 148),\n",
              " (('z', 'y'), 147),\n",
              " (('n', 'z'), 145),\n",
              " (('y', 'u'), 141),\n",
              " (('r', 'n'), 140),\n",
              " (('o', 'b'), 140),\n",
              " (('k', 'l'), 139),\n",
              " (('m', 'u'), 139),\n",
              " (('l', 'd'), 138),\n",
              " (('h', 'n'), 138),\n",
              " (('u', 'd'), 136),\n",
              " (('<START>', 'x'), 134),\n",
              " (('t', 'l'), 134),\n",
              " (('a', 'f'), 134),\n",
              " (('o', 'e'), 132),\n",
              " (('e', 'x'), 132),\n",
              " (('e', 'g'), 125),\n",
              " (('f', 'e'), 123),\n",
              " (('z', 'l'), 123),\n",
              " (('u', 'i'), 121),\n",
              " (('v', 'y'), 121),\n",
              " (('e', 'b'), 121),\n",
              " (('r', 'h'), 121),\n",
              " (('j', 'i'), 119),\n",
              " (('o', 't'), 118),\n",
              " (('d', 'h'), 118),\n",
              " (('h', 'm'), 117),\n",
              " (('c', 'l'), 116),\n",
              " (('o', 'o'), 115),\n",
              " (('y', 'c'), 115),\n",
              " (('o', 'w'), 114),\n",
              " (('o', 'c'), 114),\n",
              " (('f', 'r'), 114),\n",
              " (('b', '<END>'), 114),\n",
              " (('m', 'b'), 112),\n",
              " (('z', 'o'), 110),\n",
              " (('i', 'b'), 110),\n",
              " (('i', 'u'), 109),\n",
              " (('k', 'r'), 109),\n",
              " (('g', '<END>'), 108),\n",
              " (('y', 'v'), 106),\n",
              " (('t', 'z'), 105),\n",
              " (('b', 'o'), 105),\n",
              " (('c', 'y'), 104),\n",
              " (('y', 't'), 104),\n",
              " (('u', 'b'), 103),\n",
              " (('u', 'c'), 103),\n",
              " (('x', 'a'), 103),\n",
              " (('b', 'l'), 103),\n",
              " (('o', 'y'), 103),\n",
              " (('x', 'i'), 102),\n",
              " (('i', 'f'), 101),\n",
              " (('r', 'c'), 99),\n",
              " (('c', '<END>'), 97),\n",
              " (('m', 'r'), 97),\n",
              " (('n', 'u'), 96),\n",
              " (('o', 'p'), 95),\n",
              " (('i', 'h'), 95),\n",
              " (('k', 's'), 95),\n",
              " (('l', 's'), 94),\n",
              " (('u', 'k'), 93),\n",
              " (('<START>', 'q'), 92),\n",
              " (('d', 'u'), 92),\n",
              " (('s', 'm'), 90),\n",
              " (('r', 'k'), 90),\n",
              " (('i', 'x'), 89),\n",
              " (('v', '<END>'), 88),\n",
              " (('y', 'k'), 86),\n",
              " (('u', 'w'), 86),\n",
              " (('g', 'u'), 85),\n",
              " (('b', 'y'), 83),\n",
              " (('e', 'p'), 83),\n",
              " (('g', 'o'), 83),\n",
              " (('s', 'k'), 82),\n",
              " (('u', 't'), 82),\n",
              " (('a', 'p'), 82),\n",
              " (('e', 'f'), 82),\n",
              " (('i', 'i'), 82),\n",
              " (('r', 'v'), 80),\n",
              " (('f', '<END>'), 80),\n",
              " (('t', 'u'), 78),\n",
              " (('y', 'z'), 78),\n",
              " (('<START>', 'u'), 78),\n",
              " (('l', 't'), 77),\n",
              " (('r', 'g'), 76),\n",
              " (('c', 'r'), 76),\n",
              " (('i', 'j'), 76),\n",
              " (('w', 'y'), 73),\n",
              " (('z', 'u'), 73),\n",
              " (('l', 'v'), 72),\n",
              " (('h', 't'), 71),\n",
              " (('j', '<END>'), 71),\n",
              " (('x', 't'), 70),\n",
              " (('o', 'i'), 69),\n",
              " (('e', 'u'), 69),\n",
              " (('o', 'k'), 68),\n",
              " (('b', 'd'), 65),\n",
              " (('a', 'o'), 63),\n",
              " (('p', 'i'), 61),\n",
              " (('s', 'c'), 60),\n",
              " (('d', 'l'), 60),\n",
              " (('l', 'm'), 60),\n",
              " (('a', 'q'), 60),\n",
              " (('f', 'o'), 60),\n",
              " (('p', 'o'), 59),\n",
              " (('n', 'k'), 58),\n",
              " (('w', 'n'), 58),\n",
              " (('u', 'h'), 58),\n",
              " (('e', 'j'), 55),\n",
              " (('n', 'v'), 55),\n",
              " (('s', 'r'), 55),\n",
              " (('o', 'z'), 54),\n",
              " (('i', 'p'), 53),\n",
              " (('l', 'b'), 52),\n",
              " (('i', 'q'), 52),\n",
              " (('w', '<END>'), 51),\n",
              " (('m', 'c'), 51),\n",
              " (('s', 'p'), 51),\n",
              " (('e', 'w'), 50),\n",
              " (('k', 'u'), 50),\n",
              " (('v', 'r'), 48),\n",
              " (('u', 'g'), 47),\n",
              " (('o', 'x'), 45),\n",
              " (('u', 'z'), 45),\n",
              " (('z', 'z'), 45),\n",
              " (('j', 'h'), 45),\n",
              " (('b', 'u'), 45),\n",
              " (('o', 'g'), 44),\n",
              " (('n', 'r'), 44),\n",
              " (('f', 'f'), 44),\n",
              " (('n', 'j'), 44),\n",
              " (('z', 'h'), 43),\n",
              " (('c', 'c'), 42),\n",
              " (('r', 'b'), 41),\n",
              " (('x', 'o'), 41),\n",
              " (('b', 'h'), 41),\n",
              " (('p', 'p'), 39),\n",
              " (('x', 'l'), 39),\n",
              " (('h', 'v'), 39),\n",
              " (('b', 'b'), 38),\n",
              " (('m', 'p'), 38),\n",
              " (('x', 'x'), 38),\n",
              " (('u', 'v'), 37),\n",
              " (('x', 'e'), 36),\n",
              " (('w', 'o'), 36),\n",
              " (('c', 't'), 35),\n",
              " (('z', 'm'), 35),\n",
              " (('t', 's'), 35),\n",
              " (('m', 's'), 35),\n",
              " (('c', 'u'), 35),\n",
              " (('o', 'f'), 34),\n",
              " (('u', 'x'), 34),\n",
              " (('k', 'w'), 34),\n",
              " (('p', '<END>'), 33),\n",
              " (('g', 'l'), 32),\n",
              " (('z', 'r'), 32),\n",
              " (('d', 'n'), 31),\n",
              " (('g', 't'), 31),\n",
              " (('g', 'y'), 31),\n",
              " (('h', 's'), 31),\n",
              " (('x', 's'), 31),\n",
              " (('g', 's'), 30),\n",
              " (('x', 'y'), 30),\n",
              " (('y', 'g'), 30),\n",
              " (('d', 'm'), 30),\n",
              " (('d', 's'), 29),\n",
              " (('h', 'k'), 29),\n",
              " (('y', 'x'), 28),\n",
              " (('q', '<END>'), 28),\n",
              " (('g', 'n'), 27),\n",
              " (('y', 'b'), 27),\n",
              " (('g', 'w'), 26),\n",
              " (('n', 'h'), 26),\n",
              " (('k', 'n'), 26),\n",
              " (('g', 'g'), 25),\n",
              " (('d', 'g'), 25),\n",
              " (('l', 'c'), 25),\n",
              " (('r', 'j'), 25),\n",
              " (('w', 'u'), 25),\n",
              " (('l', 'k'), 24),\n",
              " (('m', 'd'), 24),\n",
              " (('s', 'w'), 24),\n",
              " (('s', 'n'), 24),\n",
              " (('h', 'd'), 24),\n",
              " (('w', 'h'), 23),\n",
              " (('y', 'j'), 23),\n",
              " (('y', 'y'), 23),\n",
              " (('r', 'z'), 23),\n",
              " (('d', 'w'), 23),\n",
              " (('w', 'r'), 22),\n",
              " (('t', 'n'), 22),\n",
              " (('l', 'f'), 22),\n",
              " (('y', 'h'), 22),\n",
              " (('r', 'w'), 21),\n",
              " (('s', 'b'), 21),\n",
              " (('m', 'n'), 20),\n",
              " (('f', 'l'), 20),\n",
              " (('w', 's'), 20),\n",
              " (('k', 'k'), 20),\n",
              " (('h', 'z'), 20),\n",
              " (('g', 'd'), 19),\n",
              " (('l', 'h'), 19),\n",
              " (('n', 'm'), 19),\n",
              " (('x', 'z'), 19),\n",
              " (('u', 'f'), 19),\n",
              " (('f', 't'), 18),\n",
              " (('l', 'r'), 18),\n",
              " (('p', 't'), 17),\n",
              " (('t', 'c'), 17),\n",
              " (('k', 't'), 17),\n",
              " (('d', 'v'), 17),\n",
              " (('u', 'p'), 16),\n",
              " (('p', 'l'), 16),\n",
              " (('l', 'w'), 16),\n",
              " (('p', 's'), 16),\n",
              " (('o', 'j'), 16),\n",
              " (('r', 'q'), 16),\n",
              " (('y', 'p'), 15),\n",
              " (('l', 'p'), 15),\n",
              " (('t', 'v'), 15),\n",
              " (('r', 'p'), 14),\n",
              " (('l', 'n'), 14),\n",
              " (('e', 'q'), 14),\n",
              " (('f', 'y'), 14),\n",
              " (('s', 'v'), 14),\n",
              " (('u', 'j'), 14),\n",
              " (('v', 'l'), 14),\n",
              " (('q', 'a'), 13),\n",
              " (('u', 'y'), 13),\n",
              " (('q', 'i'), 13),\n",
              " (('w', 'l'), 13),\n",
              " (('p', 'y'), 12),\n",
              " (('y', 'f'), 12),\n",
              " (('c', 'q'), 11),\n",
              " (('j', 'r'), 11),\n",
              " (('n', 'w'), 11),\n",
              " (('n', 'f'), 11),\n",
              " (('t', 'w'), 11),\n",
              " (('m', 'z'), 11),\n",
              " (('u', 'o'), 10),\n",
              " (('f', 'u'), 10),\n",
              " (('l', 'z'), 10),\n",
              " (('h', 'w'), 10),\n",
              " (('u', 'q'), 10),\n",
              " (('j', 'y'), 10),\n",
              " (('s', 'z'), 10),\n",
              " (('s', 'd'), 9),\n",
              " (('j', 'l'), 9),\n",
              " (('d', 'j'), 9),\n",
              " (('k', 'm'), 9),\n",
              " (('r', 'f'), 9),\n",
              " (('h', 'j'), 9),\n",
              " (('v', 'n'), 8),\n",
              " (('n', 'b'), 8),\n",
              " (('i', 'w'), 8),\n",
              " (('h', 'b'), 8),\n",
              " (('b', 's'), 8),\n",
              " (('w', 't'), 8),\n",
              " (('w', 'd'), 8),\n",
              " (('v', 'v'), 7),\n",
              " (('v', 'u'), 7),\n",
              " (('j', 's'), 7),\n",
              " (('m', 'j'), 7),\n",
              " (('f', 's'), 6),\n",
              " (('l', 'g'), 6),\n",
              " (('l', 'j'), 6),\n",
              " (('j', 'w'), 6),\n",
              " (('n', 'x'), 6),\n",
              " (('y', 'q'), 6),\n",
              " (('w', 'k'), 6),\n",
              " (('g', 'm'), 6),\n",
              " (('x', 'u'), 5),\n",
              " (('m', 'h'), 5),\n",
              " (('m', 'l'), 5),\n",
              " (('j', 'm'), 5),\n",
              " (('c', 's'), 5),\n",
              " (('j', 'v'), 5),\n",
              " (('n', 'p'), 5),\n",
              " (('d', 'f'), 5),\n",
              " (('x', 'd'), 5),\n",
              " (('z', 'b'), 4),\n",
              " (('f', 'n'), 4),\n",
              " (('x', 'c'), 4),\n",
              " (('m', 't'), 4),\n",
              " (('t', 'm'), 4),\n",
              " (('z', 'n'), 4),\n",
              " (('z', 't'), 4),\n",
              " (('p', 'u'), 4),\n",
              " (('c', 'z'), 4),\n",
              " (('b', 'n'), 4),\n",
              " (('z', 's'), 4),\n",
              " (('f', 'w'), 4),\n",
              " (('d', 't'), 4),\n",
              " (('j', 'd'), 4),\n",
              " (('j', 'c'), 4),\n",
              " (('y', 'w'), 4),\n",
              " (('v', 'k'), 3),\n",
              " (('x', 'w'), 3),\n",
              " (('t', 'j'), 3),\n",
              " (('c', 'j'), 3),\n",
              " (('q', 'w'), 3),\n",
              " (('g', 'b'), 3),\n",
              " (('o', 'q'), 3),\n",
              " (('r', 'x'), 3),\n",
              " (('d', 'c'), 3),\n",
              " (('g', 'j'), 3),\n",
              " (('x', 'f'), 3),\n",
              " (('z', 'w'), 3),\n",
              " (('d', 'k'), 3),\n",
              " (('u', 'u'), 3),\n",
              " (('m', 'v'), 3),\n",
              " (('c', 'x'), 3),\n",
              " (('l', 'q'), 3),\n",
              " (('p', 'b'), 2),\n",
              " (('t', 'g'), 2),\n",
              " (('q', 's'), 2),\n",
              " (('t', 'x'), 2),\n",
              " (('f', 'k'), 2),\n",
              " (('b', 't'), 2),\n",
              " (('j', 'n'), 2),\n",
              " (('k', 'c'), 2),\n",
              " (('z', 'k'), 2),\n",
              " (('s', 'j'), 2),\n",
              " (('s', 'f'), 2),\n",
              " (('z', 'j'), 2),\n",
              " (('n', 'q'), 2),\n",
              " (('f', 'z'), 2),\n",
              " (('h', 'g'), 2),\n",
              " (('w', 'w'), 2),\n",
              " (('k', 'j'), 2),\n",
              " (('j', 'k'), 2),\n",
              " (('w', 'm'), 2),\n",
              " (('z', 'c'), 2),\n",
              " (('z', 'v'), 2),\n",
              " (('w', 'f'), 2),\n",
              " (('q', 'm'), 2),\n",
              " (('k', 'z'), 2),\n",
              " (('j', 'j'), 2),\n",
              " (('z', 'p'), 2),\n",
              " (('j', 't'), 2),\n",
              " (('k', 'b'), 2),\n",
              " (('m', 'w'), 2),\n",
              " (('h', 'f'), 2),\n",
              " (('c', 'g'), 2),\n",
              " (('t', 'f'), 2),\n",
              " (('h', 'c'), 2),\n",
              " (('q', 'o'), 2),\n",
              " (('k', 'd'), 2),\n",
              " (('k', 'v'), 2),\n",
              " (('s', 'g'), 2),\n",
              " (('z', 'd'), 2),\n",
              " (('q', 'r'), 1),\n",
              " (('d', 'z'), 1),\n",
              " (('p', 'j'), 1),\n",
              " (('q', 'l'), 1),\n",
              " (('p', 'f'), 1),\n",
              " (('q', 'e'), 1),\n",
              " (('b', 'c'), 1),\n",
              " (('c', 'd'), 1),\n",
              " (('m', 'f'), 1),\n",
              " (('p', 'n'), 1),\n",
              " (('w', 'b'), 1),\n",
              " (('p', 'c'), 1),\n",
              " (('h', 'p'), 1),\n",
              " (('f', 'h'), 1),\n",
              " (('b', 'j'), 1),\n",
              " (('f', 'g'), 1),\n",
              " (('z', 'g'), 1),\n",
              " (('c', 'p'), 1),\n",
              " (('p', 'k'), 1),\n",
              " (('p', 'm'), 1),\n",
              " (('x', 'n'), 1),\n",
              " (('s', 'q'), 1),\n",
              " (('k', 'f'), 1),\n",
              " (('m', 'k'), 1),\n",
              " (('x', 'h'), 1),\n",
              " (('g', 'f'), 1),\n",
              " (('v', 'b'), 1),\n",
              " (('j', 'p'), 1),\n",
              " (('g', 'z'), 1),\n",
              " (('v', 'd'), 1),\n",
              " (('d', 'b'), 1),\n",
              " (('v', 'h'), 1),\n",
              " (('h', 'h'), 1),\n",
              " (('g', 'v'), 1),\n",
              " (('d', 'q'), 1),\n",
              " (('x', 'b'), 1),\n",
              " (('w', 'z'), 1),\n",
              " (('h', 'q'), 1),\n",
              " (('j', 'b'), 1),\n",
              " (('x', 'm'), 1),\n",
              " (('w', 'g'), 1),\n",
              " (('t', 'b'), 1),\n",
              " (('z', 'x'), 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will store bigrams in a `PyTorch` tensor instead of a dictionary. Each character will be mapped to an id."
      ],
      "metadata": {
        "id": "BlyWN2QpShN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "# character mapping from string to integer\n",
        "chars = list(string.ascii_lowercase)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "stoi['<START>'] = 26\n",
        "stoi['<END>'] = 27\n",
        "stoi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50DzzfrvUifL",
        "outputId": "bd875635-8683-490d-b5ee-4c24b128c657"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 0,\n",
              " 'b': 1,\n",
              " 'c': 2,\n",
              " 'd': 3,\n",
              " 'e': 4,\n",
              " 'f': 5,\n",
              " 'g': 6,\n",
              " 'h': 7,\n",
              " 'i': 8,\n",
              " 'j': 9,\n",
              " 'k': 10,\n",
              " 'l': 11,\n",
              " 'm': 12,\n",
              " 'n': 13,\n",
              " 'o': 14,\n",
              " 'p': 15,\n",
              " 'q': 16,\n",
              " 'r': 17,\n",
              " 's': 18,\n",
              " 't': 19,\n",
              " 'u': 20,\n",
              " 'v': 21,\n",
              " 'w': 22,\n",
              " 'x': 23,\n",
              " 'y': 24,\n",
              " 'z': 25,\n",
              " '<START>': 26,\n",
              " '<END>': 27}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "SIZE = len(stoi)\n",
        "\n",
        "def get_bigrams(n):\n",
        "  bigrams = torch.zeros((SIZE, SIZE))\n",
        "  for w in words[:n]:\n",
        "    w = ['<START>'] + list(w) + ['<END>']\n",
        "    for ch1, ch2 in zip(w, w[1:]):\n",
        "      bigrams[stoi[ch1], stoi[ch2]] += 1\n",
        "  return bigrams"
      ],
      "metadata": {
        "id": "dGZJ2doITMlA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams = get_bigrams(len(words))"
      ],
      "metadata": {
        "id": "dhA9gLNycbUq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can refer to bigrams by indices and slicing. Modify `i` and `j` and run the code to get a sense of the table."
      ],
      "metadata": {
        "id": "SXXEqDD4e_n7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "itos = {ch: i for i, ch in stoi.items()} # reverse stoi\n",
        "\n",
        "i, j = 0, 1\n",
        "count = bigrams[i, j]\n",
        "print(f'({itos[i]}, {itos[j]}): {count}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMr8AigBYJUl",
        "outputId": "cc2fd18f-6262-4472-f293-981e7ab3f9c3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(a, b): 541.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:** Find the probability of a character (e.g. `b`) being the first character (hint: it will follow `<START>`)."
      ],
      "metadata": {
        "id": "hApJynGHgaBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts = bigrams[stoi['<START>']]\n",
        "probs = counts / counts.sum()\n",
        "probs[stoi['b']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLHgvyD4eX3n",
        "outputId": "e315de26-34fe-4bc7-b922-a4eda884740d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0408)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate some output we need to understand `torch.multinomial`. Let's have a simpler probability distribution for three classes (`0`, `1`, `2`). Our goal is to generate `n` samples according to the given probabilities. Setting `replacement=True` means the same class index can be picked multiple times. The higher a class' probability, the more often it is likely to appear in the samples. Rerun the cell below and notice how probabilities are related to the generated samples."
      ],
      "metadata": {
        "id": "LmWodLOzzE0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = torch.rand(3)\n",
        "p /= p.sum()\n",
        "samples = torch.multinomial(p, num_samples=10, replacement=True)\n",
        "\n",
        "print(p)\n",
        "print(samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS0RKGBa026G",
        "outputId": "efd5c063-f408-40f0-c9b0-5e341c06f426"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.5375, 0.1891, 0.2735])\n",
            "tensor([0, 0, 0, 2, 0, 1, 0, 0, 1, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we understand the logic of `torch.multinomial`, we will randomly pick a next character based on our probability distribution. The higher is the frequency of the bigram, the more likely is that the random sampler will return us that character."
      ],
      "metadata": {
        "id": "FPWa82Bi4k_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "next_char = torch.multinomial(probs, num_samples=1, replacement=True)\n",
        "next_char, itos[next_char.item()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEBjyhkqymtr",
        "outputId": "6814c205-54a4-4644-f9c5-5cbda49c58c8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([17]), 'r')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start with bigrams of `<START>`. Once we randomly generate the next character based on its probability distribution, we will start looking for bigrams starting with that generated character. This process will continue until we our sampling returns `<END>`."
      ],
      "metadata": {
        "id": "Gg2Z2Zhm5mbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " We will work with the probability matrix from now on, instead of the frequency matrix. Below, `dim=1` ensures that we sum along the row of the matrix, when `keepdim=True` keeps the extra dimension. Refer to the `PyTorch` documentation and test out different parameters."
      ],
      "metadata": {
        "id": "E4x993bsBibs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "probs = bigrams/bigrams.sum(dim=1, keepdim=True)\n",
        "probs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo3COgQ7BGbf",
        "outputId": "37b7f366-fab3-4cf8-f60d-90777cba0e37"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_names(n=10):\n",
        "  names = ''\n",
        "  for i in range(n):\n",
        "    id = stoi['<START>']\n",
        "    while id != stoi['<END>']:\n",
        "      p = probs[id]\n",
        "      next_char = torch.multinomial(p, 1, replacement=True)\n",
        "      id = next_char.item()\n",
        "      names += itos[id]\n",
        "  return names.replace(\"<END>\", \"\\n\")"
      ],
      "metadata": {
        "id": "uZWfpYGpynLD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample_names())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2DgDJCf9r3c",
        "outputId": "0185b714-a8e6-44ea-9c9c-e9e84c673be1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tarios\n",
            "caly\n",
            "kelaherthrwa\n",
            "dyaronn\n",
            "zenel\n",
            "brid\n",
            "c\n",
            "sh\n",
            "ve\n",
            "minica\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Average Negative Log-Likelihood"
      ],
      "metadata": {
        "id": "S5onO6oJEaFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can evaluate our model and determine the loss function with [likelihood](https://en.wikipedia.org/wiki/Likelihood_function). Note that our prediction probabilities are generated by simply counting bigram frequencies."
      ],
      "metadata": {
        "id": "KD25e33MFOli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1\n",
        "for w in words[:n]:\n",
        "  w = ['<START>'] + list(w) + ['<END>']\n",
        "  for ch1, ch2 in zip(w, w[1:]):\n",
        "    p = probs[stoi[ch1], stoi[ch2]]\n",
        "    print(f'{ch1, ch2}: {p.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nmUKlKzDmJc",
        "outputId": "5fe98ab4-8655-4888-e146-1bb6a65fdeaa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('<START>', 'e'): 0.0478\n",
            "('e', 'm'): 0.0377\n",
            "('m', 'm'): 0.0253\n",
            "('m', 'a'): 0.3899\n",
            "('a', '<END>'): 0.1960\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The logic of bigram model is that the probability of rare characters coming together in names (e.g. `xy`) will be much smaller than the more common cases (e.g. `na`). A better training corpus captures more realistic character transitions and assigns higher probabilities to frequently seen patterns.\n",
        "\n",
        "Since the model assumes that each character depends only on the previous one (Markov assumption), the joint probability of a sequence is the product of all conditional probabilities:\n",
        "\n",
        "$P(c_1, c_2, \\ldots, c_n) = P(c_1) \\cdot P(c_2 \\mid c_1) \\cdot P(c_3 \\mid c_2) \\cdots P(c_n \\mid c_{n-1})$\n",
        "\n",
        "**Likelihood** estimates this quality for our model by multiplying all prediction probabilities. Higher is the joint probability, the better is model's prediction quality. However, direct multiplication may have the following issue:"
      ],
      "metadata": {
        "id": "_aGH1RFuKyg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 5\n",
        "for word in words[:n]:\n",
        "  likelihood = 1.0\n",
        "  w = ['<START>'] + list(word) + ['<END>']\n",
        "  for ch1, ch2 in zip(w, w[1:]):\n",
        "    p = probs[stoi[ch1], stoi[ch2]]\n",
        "    likelihood *= p\n",
        "  print(f'Model predicts {word} is {likelihood:.9f} likely')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lj0Sd4y1JoXy",
        "outputId": "c965440a-3b4a-4423-f159-616df9bc4417"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model predicts emma is 0.000003478 likely\n",
            "Model predicts olivia is 0.000000025 likely\n",
            "Model predicts ava is 0.000165674 likely\n",
            "Model predicts isabella is 0.000000000 likely\n",
            "Model predicts sophia is 0.000000026 likely\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** How to fix the issue above?"
      ],
      "metadata": {
        "id": "y9jrUH8oe0tm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen, the result of chained multiplication is a very small number (somewhat resembling vanishing gradient problem). To resolve this issue, individual probabilities between `0` and `1` are mapped to a `log` function domain (-$\\infty$, 0]. Logarithm function is monotonic (preserves order): maximum probability is mapped to `0`, smaller probabilities are mapped to bigger negative values."
      ],
      "metadata": {
        "id": "0k717SP2QEAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "p = np.linspace(0.001, 1, 200)\n",
        "log_p = np.log(p)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(p, log_p)\n",
        "plt.title(\"Natural Log Function\")\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "mCfjzk-jM1yW",
        "outputId": "c6d64c1a-ce39-4d1c-b1f5-f4108d963742"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1\n",
        "for w in words[:n]:\n",
        "  w = ['<START>'] + list(w) + ['<END>']\n",
        "  for ch1, ch2 in zip(w, w[1:]):\n",
        "    p = probs[stoi[ch1], stoi[ch2]]\n",
        "    log_p = torch.log(p)\n",
        "    print(f'{ch1, ch2}: {p.item():.4f} | {log_p.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIt4bau7j9sy",
        "outputId": "35c79f6f-b719-4b12-9081-8cafa5f76f01"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('<START>', 'e'): 0.0478 | -3.0408\n",
            "('e', 'm'): 0.0377 | -3.2793\n",
            "('m', 'm'): 0.0253 | -3.6772\n",
            "('m', 'a'): 0.3899 | -0.9418\n",
            "('a', '<END>'): 0.1960 | -1.6299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Log-likelihood** also has an advantage of making calculations and hence optimization (calculation of gradients) faster due to the product rule:\n",
        "\n",
        "$\\log P(c_1, c_2, \\ldots, c_n) = \\log P(c_1) + \\log P(c_2 \\mid c_1) + \\log P(c_3 \\mid c_2) + \\cdots + \\log P(c_n \\mid c_{n-1})$"
      ],
      "metadata": {
        "id": "2uRFxvmShJMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 5\n",
        "for word in words[:n]:\n",
        "  log_likelihood = 0.0\n",
        "  w = ['<START>'] + list(word) + ['<END>']\n",
        "  for ch1, ch2 in zip(w, w[1:]):\n",
        "    p = probs[stoi[ch1], stoi[ch2]]\n",
        "    log_p = torch.log(p)\n",
        "    log_likelihood += log_p\n",
        "  print(f'Model predicts {word} is {log_likelihood} likely')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Hmt2hxFRP1i",
        "outputId": "f79c9aa7-c714-495e-9b5f-1ec273f6bd80"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model predicts emma is -12.568990707397461 likely\n",
            "Model predicts olivia is -17.511159896850586 likely\n",
            "Model predicts ava is -8.705486297607422 likely\n",
            "Model predicts isabella is -21.5141544342041 likely\n",
            "Model predicts sophia is -17.468196868896484 likely\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As optimization algorithms usually strive for minimizing the loss, it makes sense to invert the negative values of the log-likelihood to be positive. We can generate a single loss value by **averaging negative log-likelihoods** across all the samples."
      ],
      "metadata": {
        "id": "PkAXyDyYSvPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_likelihood = 0.0\n",
        "for word in words:\n",
        "  w = ['<START>'] + list(word) + ['<END>']\n",
        "  for ch1, ch2 in zip(w, w[1:]):\n",
        "    p = probs[stoi[ch1], stoi[ch2]]\n",
        "    log_p = torch.log(p)\n",
        "    log_likelihood += log_p\n",
        "loss = -log_likelihood / len(words)\n",
        "print(f'Loss: {loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eiz34gvpSrhb",
        "outputId": "452aa3d8-e95d-46df-ea59-7735c6eb7ab8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 17.478591918945312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we know, logarithmic function is undefined at `0`, which we need to take into consideration. Consider the case when a character combination has never occured in our training data."
      ],
      "metadata": {
        "id": "ILy0aHfZyYZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in ['jq']:\n",
        "  log_likelihood = 0.0\n",
        "  w = ['<START>'] + list(word) + ['<END>']\n",
        "  for ch1, ch2 in zip(w, w[1:]):\n",
        "    p = probs[stoi[ch1], stoi[ch2]]\n",
        "    log_p = torch.log(p)\n",
        "    log_likelihood += log_p\n",
        "  print(f'Model predicts {word} is {log_likelihood} likely')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hwv-yoYCx3iH",
        "outputId": "e4468f70-ba17-404a-cd5b-ab089fa07c50"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model predicts jq is -inf likely\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In case any character sequence in string will return infinite likelihood, it will lead to infinite loss as well, which is undesirable.\n",
        "\n",
        "**Question:** How to avoid infinite loss?\n",
        "\n",
        "**Model-smoothing** is a simple technique, which aims to assign a minimal non-zero probability to cases leading to infinite likelihood. Run the next cell and replicate the experiments above to see the outcome."
      ],
      "metadata": {
        "id": "6GyeYf4SzhYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams = bigrams + 1  # model smoothing avoids zero probabilities\n",
        "probs = bigrams/bigrams.sum(dim=1, keepdim=True)"
      ],
      "metadata": {
        "id": "LGYY1SoKy0_b"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network Bigram Model"
      ],
      "metadata": {
        "id": "t7NqK9244Mwp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our frequency-based bigram model didn't perform well due its simplicity. We will now build a neural network-based bigram model with the aim of increasing individual bigram prediction probabilities (recall that likelihood was calculated by multiplying conditional probabilities). Instead of counting bigrams in our training set, we will learn parameters leading to reduced loss. We will now rewrite our `get_bigrams()` function to suit the training of neural network model, where the label of each character will be the next character."
      ],
      "metadata": {
        "id": "MITGK6Jp4uLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bigrams(n):\n",
        "  X, Y = [], []\n",
        "  for w in words[:n]:\n",
        "    w = ['<START>'] + list(w) + ['<END>']\n",
        "    for ch1, ch2 in zip(w, w[1:]):\n",
        "      X.append(stoi[ch1])\n",
        "      Y.append(stoi[ch2])\n",
        "  return torch.tensor(X), torch.tensor(Y)"
      ],
      "metadata": {
        "id": "Ol-K-lU_4PT_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = get_bigrams(1)\n",
        "X, Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Evsjo8Qq8Xqm",
        "outputId": "ed4684ec-2b28-408a-8d34-80e1dff4ac96"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([26,  4, 12, 12,  0]), tensor([ 4, 12, 12,  0, 27]))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[itos[x.item()] for x in X], [itos[y.item()] for y in Y]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv5A-mQN8fC5",
        "outputId": "af9d35e3-a8ae-46e8-f5b9-ffb477af0a01"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['<START>', 'e', 'm', 'm', 'a'], ['e', 'm', 'm', 'a', '<END>'])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will one-hot encode our data with the `torch.nn.functional` module function, in order to not inject unnecessary numerical pattern to our data."
      ],
      "metadata": {
        "id": "9XExmlWJ-6_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "X_train = F.one_hot(X, num_classes=SIZE).float()\n",
        "y_train = F.one_hot(Y, num_classes=SIZE).float()"
      ],
      "metadata": {
        "id": "VoKzrEbg_Xke"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X_train);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "id": "N-BjbKRY_oIV",
        "outputId": "815fbfdb-4840-4898-87b8-a199df745450"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAACECAYAAADMdmKyAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAADb5JREFUeJzt3X9oleXDx/HPNrfjj86OzrUfp+mcWkrNTVK3xCcTNpwWkukfVv6xhhjVmThHJQt0CcHCIKSSjKD8x18JLUke/CLLTYL5g4mokPvqHh9czG0puM2Zc+1czx89ni8nbXq26+zefXq/4Iad+1zc58PFVefjfe5z7jhjjBEAAIAF8U4HAAAAsYNiAQAArKFYAAAAaygWAADAGooFAACwhmIBAACsoVgAAABrKBYAAMCaMSP5YsFgUG1tbfJ6vYqLixvJlwYAAENkjFFPT4/8fr/i4wc/JzGixaKtrU1TpkwZyZcEAACWtLa2Kisra9AxI1osvF6vJOm/9KLGKHEkXxoAANep/ff5YR/jlafmDPsYf6hfP+u/Q+/jgxnRYnHv448xStSYOIoFAACDSfYO/1JIK++3/39XsUe5jIGLNwEAgDVDKhY7d+7UtGnTNHbsWBUWFurUqVO2cwEAABeKuFgcOHBAlZWVqq6u1pkzZ5Sfn6+SkhJ1dnZGIx8AAHCRiIvFp59+qvXr16usrExPP/20du3apfHjx+ubb76JRj4AAOAiERWLu3fvqqmpScXFxf85QHy8iouL1djYaD0cAABwl4i+FXL9+nUNDAwoPT09bH96erouXrx43/i+vj719fWFHnd3dw8xJgAAcIOofiukpqZGPp8vtPHjWAAAxLaIikVqaqoSEhLU0dERtr+jo0MZGRn3ja+qqlJXV1doa21tHV5aAAAwqkVULJKSkjRv3jzV1dWF9gWDQdXV1WnhwoX3jfd4PEpOTg7bAABA7Ir4lzcrKytVWlqq+fPnq6CgQDt27FBvb6/KysqikQ8AALhIxMVizZo1+u2337R161a1t7dr7ty5OnLkyH0XdAIAgH+eId0rpLy8XOXl5bazAAAAl+NeIQAAwBqKBQAAsGZEb5uOwf2r7ayV45T451o5DgDAWW78/zlnLAAAgDUUCwAAYA3FAgAAWEOxAAAA1lAsAACANRQLAABgDcUCAABYQ7EAAADWUCwAAIA1FAsAAGANxQIAAFhDsQAAANZQLAAAgDUUCwAAYA3FAgAAWEOxAAAA1lAsAACANWOcDoD/KPHPdToCXOhfbWeHfQzWHgBbOGMBAACsoVgAAABrKBYAAMAaigUAALAmomJRU1OjBQsWyOv1Ki0tTStXrlRzc3O0sgEAAJeJqFg0NDQoEAjoxIkTOnr0qPr7+7V06VL19vZGKx8AAHCRiL5ueuTIkbDHu3fvVlpampqamrR48WKrwQAAgPsM63csurq6JEkpKSkPfL6vr099fX2hx93d3cN5OQAAMMoN+eLNYDCoiooKLVq0SLm5uQ8cU1NTI5/PF9qmTJky5KAAAGD0G3KxCAQCunDhgvbv3/+3Y6qqqtTV1RXaWltbh/pyAADABYb0UUh5ebkOHz6s48ePKysr62/HeTweeTyeIYcDAADuElGxMMZow4YNqq2tVX19vXJycqKVCwAAuFBExSIQCGjv3r06dOiQvF6v2tvbJUk+n0/jxo2LSkAAAOAeEV1j8eWXX6qrq0tLlixRZmZmaDtw4EC08gEAABeJ+KMQAACAv8O9QgAAgDUUCwAAYA3FAgAAWEOxAAAA1lAsAACANRQLAABgDcUCAABYQ7EAAADWUCwAAIA1FAsAAGANxQIAAFhDsQAAANZQLAAAgDUUCwAAYA3FAgAAWEOxAAAA1lAsAACANRQLAABgDcUCAABYM8aJF63993kle4fXaUr8c+2EAVyO/xYAjCacsQAAANZQLAAAgDUUCwAAYA3FAgAAWDOsYvHxxx8rLi5OFRUVluIAAAA3G3KxOH36tL766ivl5eXZzAMAAFxsSMXi1q1bWrt2rb7++mtNmjTJdiYAAOBSQyoWgUBAL730koqLiwcd19fXp+7u7rANAADEroh/IGv//v06c+aMTp8+/dCxNTU12rZt25CCAQAA94nojEVra6s2btyoPXv2aOzYsQ8dX1VVpa6urtDW2to65KAAAGD0i+iMRVNTkzo7O/Xss8+G9g0MDOj48eP64osv1NfXp4SEhNBzHo9HHo/HXloAADCqRVQsioqKdP78+bB9ZWVlmj17tjZv3hxWKgAAwD9PRMXC6/UqNzc3bN+ECRM0efLk+/YDAIB/Hn55EwAAWDPs26bX19dbiAEAAGIBZywAAIA1wz5jEQljjCSp+1Zw2Mf6w/QP+xgAAODh/tCf77n33scHM6LFoqenR5KU/ez/Wjja/1g4BgAAeFQ9PT3y+XyDjokzj1I/LAkGg2pra5PX61VcXNwDx3R3d2vKlClqbW1VcnLySEX7R2GOo4v5jT7mOPqY4+hy2/waY9TT0yO/36/4+MGvohjRMxbx8fHKysp6pLHJycmumGw3Y46ji/mNPuY4+pjj6HLT/D7sTMU9XLwJAACsoVgAAABrRl2x8Hg8qq6u5h4jUcQcRxfzG33McfQxx9EVy/M7ohdvAgCA2DbqzlgAAAD3olgAAABrKBYAAMAaigUAALBm1BWLnTt3atq0aRo7dqwKCwt16tQppyPFjA8//FBxcXFh2+zZs52O5VrHjx/XihUr5Pf7FRcXpx9++CHseWOMtm7dqszMTI0bN07FxcW6dOmSM2Fd6mFz/MYbb9y3ppctW+ZMWBeqqanRggUL5PV6lZaWppUrV6q5uTlszJ07dxQIBDR58mQ99thjWr16tTo6OhxK7C6PMr9Lliy5bw2/9dZbDiW2Y1QViwMHDqiyslLV1dU6c+aM8vPzVVJSos7OTqejxYxnnnlG165dC20///yz05Fcq7e3V/n5+dq5c+cDn9++fbs+++wz7dq1SydPntSECRNUUlKiO3fujHBS93rYHEvSsmXLwtb0vn37RjChuzU0NCgQCOjEiRM6evSo+vv7tXTpUvX29obGbNq0ST/++KMOHjyohoYGtbW1adWqVQ6mdo9HmV9JWr9+fdga3r59u0OJLTGjSEFBgQkEAqHHAwMDxu/3m5qaGgdTxY7q6mqTn5/vdIyYJMnU1taGHgeDQZORkWE++eST0L6bN28aj8dj9u3b50BC9/vrHBtjTGlpqXn55ZcdyROLOjs7jSTT0NBgjPlzzSYmJpqDBw+Gxvzyyy9GkmlsbHQqpmv9dX6NMeaFF14wGzdudC5UFIyaMxZ3795VU1OTiouLQ/vi4+NVXFysxsZGB5PFlkuXLsnv92v69Olau3atrl696nSkmHTlyhW1t7eHrWefz6fCwkLWs2X19fVKS0vTrFmz9Pbbb+vGjRtOR3Ktrq4uSVJKSookqampSf39/WHrePbs2Zo6dSrreAj+Or/37NmzR6mpqcrNzVVVVZVu377tRDxrRvQmZIO5fv26BgYGlJ6eHrY/PT1dFy9edChVbCksLNTu3bs1a9YsXbt2Tdu2bdPzzz+vCxcuyOv1Oh0vprS3t0vSA9fzvecwfMuWLdOqVauUk5OjlpYWffDBB1q+fLkaGxuVkJDgdDxXCQaDqqio0KJFi5Sbmyvpz3WclJSkiRMnho1lHUfuQfMrSa+//rqys7Pl9/t17tw5bd68Wc3Nzfr+++8dTDs8o6ZYIPqWL18e+jsvL0+FhYXKzs7Wd999p3Xr1jmYDBiaV199NfT3nDlzlJeXpxkzZqi+vl5FRUUOJnOfQCCgCxcucN1VlPzd/L755puhv+fMmaPMzEwVFRWppaVFM2bMGOmYVoyaj0JSU1OVkJBw39XGHR0dysjIcChVbJs4caKeeuopXb582ekoMefemmU9j6zp06crNTWVNR2h8vJyHT58WMeOHVNWVlZof0ZGhu7evaubN2+GjWcdR+bv5vdBCgsLJcnVa3jUFIukpCTNmzdPdXV1oX3BYFB1dXVauHChg8li161bt9TS0qLMzEyno8ScnJwcZWRkhK3n7u5unTx5kvUcRb/++qtu3LjBmn5ExhiVl5ertrZWP/30k3JycsKenzdvnhITE8PWcXNzs65evco6fgQPm98HOXv2rCS5eg2Pqo9CKisrVVpaqvnz56ugoEA7duxQb2+vysrKnI4WE959912tWLFC2dnZamtrU3V1tRISEvTaa685Hc2Vbt26FfaviitXrujs2bNKSUnR1KlTVVFRoY8++khPPvmkcnJytGXLFvn9fq1cudK50C4z2BynpKRo27ZtWr16tTIyMtTS0qL3339fM2fOVElJiYOp3SMQCGjv3r06dOiQvF5v6LoJn8+ncePGyefzad26daqsrFRKSoqSk5O1YcMGLVy4UM8995zD6Ue/h81vS0uL9u7dqxdffFGTJ0/WuXPntGnTJi1evFh5eXkOpx8Gp7+W8leff/65mTp1qklKSjIFBQXmxIkTTkeKGWvWrDGZmZkmKSnJPPHEE2bNmjXm8uXLTsdyrWPHjhlJ922lpaXGmD+/crplyxaTnp5uPB6PKSoqMs3Nzc6GdpnB5vj27dtm6dKl5vHHHzeJiYkmOzvbrF+/3rS3tzsd2zUeNLeSzLfffhsa8/vvv5t33nnHTJo0yYwfP9688sor5tq1a86FdpGHze/Vq1fN4sWLTUpKivF4PGbmzJnmvffeM11dXc4GHyZumw4AAKwZNddYAAAA96NYAAAAaygWAADAGooFAACwhmIBAACsoVgAAABrKBYAAMAaigUAALCGYgEAAKyhWAAAAGsoFgAAwBqKBQAAsOb/ABnhdekYVqnyAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now generate weights for each character and find their linear transformation."
      ],
      "metadata": {
        "id": "nPNQ4kJoEJLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.randn((SIZE, 1))\n",
        "X_train @ W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frt3wwQKALZA",
        "outputId": "8e284c08-66f2-4dc4-925a-15a07f3ccdf7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.5013],\n",
              "        [-0.8258],\n",
              "        [ 1.0634],\n",
              "        [ 1.0634],\n",
              "        [ 0.9002]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each input character, our goal is to predict not a single probability, but probabilities for all possible output characters. Hence, we will update our weight matrix to correspond to both input and output. We will set `requires_grad=True` for future gradient calculation."
      ],
      "metadata": {
        "id": "VL30yn6rGACY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.randn((SIZE, SIZE), requires_grad=True)\n",
        "(X_train @ W).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "534G6dEpAxhv",
        "outputId": "4811bab8-e2e0-4e7b-ef7a-73a008575a5c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that `torch.randn()` function is generating weight values corresponding to Guassian distribution. Our goal is to map these values to all be positive, so that we can interpret the ouput as probabilities later on. The idea is similar to `log()` function previously. Here, the output values lower than zero will be mapped to be below `1` approaching `0`, when positive values will grow towards infinity."
      ],
      "metadata": {
        "id": "_lV9p_BnLDCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(-4, 4, 200)\n",
        "exp_x = np.exp(x)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(x, exp_x)\n",
        "plt.title(\"Exponentiation Function\")\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "O85rS5v2Hc2e",
        "outputId": "23aa7616-5fd4-471e-d6a1-59179543f4af"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We call the raw scores we obtain after a linear transformation **logits** (log-counts). These values are real numbers, not constrained to be positive or normalized. To interpret them as prediction probabilities, we apply the exponential function to map them to the positive domain. The output of exponentiation can be interpreted as unnormalized character frequencies or relative likelihoods. When we normalize these exponentials (i.e. divide by their total sum), we obtain values that sum to `1.0`, forming a proper probability distribution. This is exactly what we want. Note that all these functions are differentiable and applying exponentiation and normalization is exactly what **softmax** function does:\n",
        "\n",
        "$\\text{softmax}(z_i) = \\frac{\\exp(z_i)}{\\sum_{j=1}^{K} \\exp(z_j)}$\n"
      ],
      "metadata": {
        "id": "CT9RJKroOV0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = X_train @ W\n",
        "\n",
        "# softmax\n",
        "counts = torch.exp(logits)\n",
        "probs = counts / counts.sum(dim=1, keepdim=True)\n",
        "\n",
        "probs[0], probs[0].sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwetEfaeKl2w",
        "outputId": "cdd6a3d1-a814-4cb0-c432-95841f799a06"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.0471, 0.0249, 0.0381, 0.0411, 0.0384, 0.0678, 0.0478, 0.0210, 0.0155,\n",
              "         0.0191, 0.0920, 0.0500, 0.0138, 0.0090, 0.0042, 0.0120, 0.1256, 0.0500,\n",
              "         0.0855, 0.0199, 0.0043, 0.0082, 0.0215, 0.0744, 0.0232, 0.0054, 0.0234,\n",
              "         0.0168], grad_fn=<SelectBackward0>),\n",
              " tensor(1.0000, grad_fn=<SumBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:** Calculate the average negative log-likelihood (loss) of the model."
      ],
      "metadata": {
        "id": "y3huHk7XVSNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_probs = probs[torch.arange(len(Y)), Y]\n",
        "loss = -pred_probs.log().mean()\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75DNxXI6Q_pe",
        "outputId": "01f1acc5-aa1a-460d-a795-ec2b86305b90"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.5038, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know how to call backward pass and optimize our model from previous lectures. We will combine all the steps, train our model on the whole dataset, and backpropagate through our network."
      ],
      "metadata": {
        "id": "y3n1uOPfXquD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = get_bigrams(len(words))\n",
        "\n",
        "X_train = F.one_hot(X, num_classes=SIZE).float()\n",
        "y_train = F.one_hot(Y, num_classes=SIZE).float()\n",
        "\n",
        "W = torch.randn((SIZE, SIZE), requires_grad=True)"
      ],
      "metadata": {
        "id": "NFdSxfoxYY6l"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100\n",
        "learning_rate = 1\n",
        "lambda_ = 0.01\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  # forward pass\n",
        "  logits = X_train @ W\n",
        "  counts = torch.exp(logits)\n",
        "  probs = counts / counts.sum(dim=1, keepdim=True)\n",
        "  pred_probs = probs[torch.arange(len(Y)), Y]\n",
        "\n",
        "  l2 = (W**2).sum() # regularization\n",
        "  loss = -pred_probs.log().mean() + lambda_ * l2.sum()\n",
        "\n",
        "  # backward pass\n",
        "  W.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # optimization\n",
        "  W.data -= learning_rate * W.grad\n",
        "\n",
        "  if (epoch + 1) % 10 == 0:\n",
        "    print(f'{epoch+1}/{num_epochs}, loss: {loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ptbFRHbZtjF",
        "outputId": "a36ccfa3-328c-42d6-8433-95864cbcc057"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/100, loss: 9.035313606262207\n",
            "20/100, loss: 7.005525588989258\n",
            "30/100, loss: 5.685214042663574\n",
            "40/100, loss: 4.8257904052734375\n",
            "50/100, loss: 4.2659807205200195\n",
            "60/100, loss: 3.901088237762451\n",
            "70/100, loss: 3.6630942821502686\n",
            "80/100, loss: 3.5077712535858154\n",
            "90/100, loss: 3.406341552734375\n",
            "100/100, loss: 3.3400678634643555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network N-gram Model"
      ],
      "metadata": {
        "id": "IdiYjJizCoYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follows the implementation of Yoshua Bengio, Rjean Ducharme, Pascal Vincent, and Christian Janvin: [A neural probabilistic language model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf), based on Andrej Karpathy's [building makemore: part 2](https://www.youtube.com/watch?v=TCH_1BHY58I&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=4)."
      ],
      "metadata": {
        "id": "xCzBsK-QB3-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will modify the `get_bigrams()` function to include custom `block_size`, which simply implies `N-1` of `N-gram` (bigram has the block size of `1`)."
      ],
      "metadata": {
        "id": "k8eUBZJwGZ4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ngrams(end, start=0, block_size=3):\n",
        "  X, Y = [], []\n",
        "  for w in words[start:end]:\n",
        "    context = ['<START>'] * block_size\n",
        "    for ch in list(w) + ['<END>']:\n",
        "      X.append([stoi[c] for c in context])\n",
        "      Y.append(stoi[ch])\n",
        "      context = context[1:] + [ch]\n",
        "  return torch.tensor(X), torch.tensor(Y)"
      ],
      "metadata": {
        "id": "jWLhb1FVMGDn"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = get_ngrams(1, block_size=3) # try different block sizes"
      ],
      "metadata": {
        "id": "R6-wF_cfHCRv"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in zip(X, Y):\n",
        "  context = [itos[i.item()] for i in x]\n",
        "  target = itos[y.item()]\n",
        "  print(f\"{context}: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDXG47bTNCma",
        "outputId": "0a11b7b1-78d9-47d7-9d57-d8ab83c72f6d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<START>', '<START>', '<START>']: e\n",
            "['<START>', '<START>', 'e']: m\n",
            "['<START>', 'e', 'm']: m\n",
            "['e', 'm', 'm']: a\n",
            "['m', 'm', 'a']: <END>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is managable to convert `28` character indices by one hot encoding them to suit a neural network. However, this approach becomes innefficient when the vocabulary size increases. What if we have `10,000` words and our goal is to predict the next word? We would have to create large vectors with lots of zeros, not only wasting resources, but also losing similarity information among tokens (dot product of any two vectors will always be `0` due to orthogonality).\n",
        "\n",
        "A different approach is to get some smaller dimensional embedding of an index. Initially, these embeddings (parameters) are random, but over the course of training, model updates them to reflect the actual usage of the words in a context. When vectors of two different characters are learned to be close to each other, then we can conclude that, in our data, these characters were in a similar context. If two characters often show up in the same positions relative to surrounding characters, their embeddings get pulled toward each other. Consider the simple case below:\n",
        "\n",
        "```python\n",
        "context1 = ['b', 'a', 'd', 'a']\n",
        "context2 = ['m', 'a', 'd', 'a']\n",
        "target = 'm'\n",
        "```\n",
        "If the training data consists of these contexts, then, in order to predict `m` for both contexts, their learned embeddings should be similar in value."
      ],
      "metadata": {
        "id": "Ri06hoIMOhZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our case, `2` dimensional embeddings will be enough. For small datasets, having high embedding dimensionality may cause overfitting. When dataset is bigger, increasing dimensions helps to learn more nuanced relationships in the data, albeit at a higher computational cost."
      ],
      "metadata": {
        "id": "8TFrznqjWiNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEf8OtHAXoP-",
        "outputId": "13948585-619f-4c3d-c503-63acbd4da089"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[26, 26, 26],\n",
              "        [26, 26,  4],\n",
              "        [26,  4, 12],\n",
              "        [ 4, 12, 12],\n",
              "        [12, 12,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "C = torch.randn((SIZE, 2))\n",
        "emb = C[X]\n",
        "emb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsxutGn-PRJY",
        "outputId": "524ef676-1536-483a-e0d6-d1ae4008b2f3"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.6723,  0.3140],\n",
              "         [-0.6723,  0.3140],\n",
              "         [-0.6723,  0.3140]],\n",
              "\n",
              "        [[-0.6723,  0.3140],\n",
              "         [-0.6723,  0.3140],\n",
              "         [-1.4631, -1.0324]],\n",
              "\n",
              "        [[-0.6723,  0.3140],\n",
              "         [-1.4631, -1.0324],\n",
              "         [ 0.6817,  1.5840]],\n",
              "\n",
              "        [[-1.4631, -1.0324],\n",
              "         [ 0.6817,  1.5840],\n",
              "         [ 0.6817,  1.5840]],\n",
              "\n",
              "        [[ 0.6817,  1.5840],\n",
              "         [ 0.6817,  1.5840],\n",
              "         [-0.8068,  0.7403]]])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will initilaize weights and biases by considering correct shape. In order to be able to use matrix multiplication, we will have to flatten embeddings as well."
      ],
      "metadata": {
        "id": "UrJfI0D9Zg6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkIABp_wahNU",
        "outputId": "88bd9b52-aced-4357-df61-e49b84d367b8"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** What do embedding dimensions correspond to?\n",
        "\n",
        "**Exercise:** Pass embeddings through a single neuron."
      ],
      "metadata": {
        "id": "WYlCXLIzakuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer_size = 100\n",
        "in_features = emb.shape[1] * emb.shape[2]\n",
        "\n",
        "W1 = torch.randn((in_features, layer_size))\n",
        "b1 = torch.randn(layer_size)\n",
        "\n",
        "out = emb.view(-1, in_features) @ W1 + b1\n",
        "act = torch.tanh(out)\n",
        "\n",
        "W1.shape, b1.shape, out.shape"
      ],
      "metadata": {
        "id": "-lx1mVhcXpES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "503b7d42-9f51-4721-8333-8ea1e08138bd"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([6, 100]), torch.Size([100]), torch.Size([5, 100]))"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb.view(-1, in_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8tO7cmFbZWL",
        "outputId": "a0961afc-f87b-4a4b-a533-be2d8484d63c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.6723,  0.3140, -0.6723,  0.3140, -0.6723,  0.3140],\n",
              "        [-0.6723,  0.3140, -0.6723,  0.3140, -1.4631, -1.0324],\n",
              "        [-0.6723,  0.3140, -1.4631, -1.0324,  0.6817,  1.5840],\n",
              "        [-1.4631, -1.0324,  0.6817,  1.5840,  0.6817,  1.5840],\n",
              "        [ 0.6817,  1.5840,  0.6817,  1.5840, -0.8068,  0.7403]])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:** Create the next (final) layer."
      ],
      "metadata": {
        "id": "UHp_iKlhcmDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W2 = torch.randn((layer_size, SIZE))\n",
        "b2 = torch.randn(SIZE)\n",
        "\n",
        "logits = act @ W2 + b2\n",
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxrHdaiZcK9V",
        "outputId": "2862dbe4-1760-4580-aec4-6bc3c7040812"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:** Calculate loss."
      ],
      "metadata": {
        "id": "vKU3zS6wdJaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts = logits.exp()\n",
        "prob = counts / counts.sum(dim=1, keepdim=True)\n",
        "prob.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O4qzNgEc8l8",
        "outputId": "c4c1515f-7c1b-4507-e2f0-acff7b462a51"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = -prob[torch.arange(prob.shape[0]), Y].log().mean()\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8NzSIpBdwEV",
        "outputId": "69b21a7e-9efb-403f-c50b-81aeee9061d4"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(16.2439)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average negative log-likelihood loss can be calculated with built-in `PyTorch` functions as well. **Cross-entropy** is simply `Softmax + Negative Log-Likelihood`."
      ],
      "metadata": {
        "id": "9hV2Q_T1g_RI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss = F.nll_loss(torch.log(prob), Y)\n",
        "loss = F.cross_entropy(logits, Y)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OSkdyw4g-ju",
        "outputId": "d0834ce8-349a-4552-c319-9bdd217c1700"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(16.2439)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:** Let's combine everything together and train on whole data."
      ],
      "metadata": {
        "id": "Am4QNshshpD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = get_ngrams(len(words))"
      ],
      "metadata": {
        "id": "J0eVIMMVmCge"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "C = torch.randn((SIZE, 2), requires_grad=True)\n",
        "emb = C[X]\n",
        "\n",
        "layer_size = 100\n",
        "in_features = emb.shape[1] * emb.shape[2]\n",
        "\n",
        "W1 = torch.randn((in_features, layer_size), requires_grad=True)\n",
        "b1 = torch.randn(layer_size, requires_grad=True)\n",
        "\n",
        "W2 = torch.randn((layer_size, SIZE), requires_grad=True)\n",
        "b2 = torch.randn(SIZE, requires_grad=True)\n",
        "\n",
        "params = [C, W1, W2, b1, b2]"
      ],
      "metadata": {
        "id": "Fq_W3WClhT2E"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X, Y, params, num_epochs=10, learning_rate=0.01):\n",
        "  C, W1, W2, b1, b2 = params\n",
        "  for epoch in range(num_epochs):\n",
        "    # forward pass\n",
        "    emb = C[X]\n",
        "    in_features = emb.shape[1] * emb.shape[2]\n",
        "    out = emb.view(-1, in_features) @ W1 + b1\n",
        "    act = torch.tanh(out)\n",
        "    logits = act @ W2 + b2\n",
        "    loss = F.cross_entropy(logits, Y)\n",
        "    print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {loss}')\n",
        "    # backward pass\n",
        "    for p in params:\n",
        "      p.grad = None\n",
        "    loss.backward()\n",
        "    # optimization\n",
        "    for p in params:\n",
        "      p.data -= learning_rate * p.grad"
      ],
      "metadata": {
        "id": "WSXw6Ddsjo1k"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(X, Y, params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9emPNn_m58o",
        "outputId": "50d9f240-3545-4323-e98d-20958c72999c"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10, Loss: 14.651238441467285\n",
            "Epoch: 2/10, Loss: 14.548416137695312\n",
            "Epoch: 3/10, Loss: 14.447701454162598\n",
            "Epoch: 4/10, Loss: 14.349052429199219\n",
            "Epoch: 5/10, Loss: 14.252448081970215\n",
            "Epoch: 6/10, Loss: 14.157876968383789\n",
            "Epoch: 7/10, Loss: 14.065322875976562\n",
            "Epoch: 8/10, Loss: 13.974783897399902\n",
            "Epoch: 9/10, Loss: 13.886248588562012\n",
            "Epoch: 10/10, Loss: 13.799703598022461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training on Mini-Batches"
      ],
      "metadata": {
        "id": "M6wlXdMaqkFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:** Notice how slow it is to train on the whole data. Find out why. Can you also train on mini-batches? See the hint below:"
      ],
      "metadata": {
        "id": "P9QzSnhInLK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "idx = torch.randint(0, X.shape[0], (batch_size,))\n",
        "idx, X[idx]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srpSYNO9ksVq",
        "outputId": "2d6001b1-24a7-4dd6-8eb1-7bcfdc6e1cd9"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([114762,  40173,  66663,  87063]),\n",
              " tensor([[ 7, 11,  4],\n",
              "         [18,  4, 24],\n",
              "         [ 0, 13,  4],\n",
              "         [12,  8, 11]]))"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run(X, Y, params, num_epochs, lr=0.1, batch_size=None):\n",
        "  C, W1, W2, b1, b2 = params\n",
        "\n",
        "  for epoch in range(1, num_epochs+1):\n",
        "    if batch_size:\n",
        "      idx = torch.randint(0, X.size(0), (batch_size,))\n",
        "      batch_X, batch_Y = X[idx], Y[idx]\n",
        "    else:\n",
        "      batch_X, batch_Y = X, Y\n",
        "\n",
        "    emb = C[batch_X]\n",
        "    in_features = emb.shape[1] * emb.shape[2]\n",
        "    out = emb.view(-1, in_features) @ W1 + b1\n",
        "    act = torch.tanh(out)\n",
        "    logits = act @ W2 + b2\n",
        "    loss = F.cross_entropy(logits, batch_Y)\n",
        "\n",
        "    if epoch % (100 if batch_size else 1) == 0:\n",
        "      print(f'Epoch {epoch}, Loss {loss.item()}')\n",
        "\n",
        "    for p in params:\n",
        "      p.grad = None\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for p in params:\n",
        "        p.data -= lr * p.grad"
      ],
      "metadata": {
        "id": "WanHatPNnziW"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run(X, Y, params, num_epochs=10, batch_size=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMKQzDuyoVYq",
        "outputId": "b5470e24-44c2-4eab-c922-77ff8c18a5fd"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss 13.715128898620605\n",
            "Epoch 2, Loss 12.94245719909668\n",
            "Epoch 3, Loss 12.326409339904785\n",
            "Epoch 4, Loss 11.779574394226074\n",
            "Epoch 5, Loss 11.269842147827148\n",
            "Epoch 6, Loss 10.805395126342773\n",
            "Epoch 7, Loss 10.449189186096191\n",
            "Epoch 8, Loss 10.27437686920166\n",
            "Epoch 9, Loss 9.558894157409668\n",
            "Epoch 10, Loss 9.190631866455078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run(X, Y, params, num_epochs=1000, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoijeUSPPx_u",
        "outputId": "44fff814-d3f9-49c5-b9bc-ab327f6ea648"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100, Loss 2.8726632595062256\n",
            "Epoch 200, Loss 2.904355525970459\n",
            "Epoch 300, Loss 3.114861488342285\n",
            "Epoch 400, Loss 2.7828195095062256\n",
            "Epoch 500, Loss 2.580034017562866\n",
            "Epoch 600, Loss 2.742938995361328\n",
            "Epoch 700, Loss 2.582310676574707\n",
            "Epoch 800, Loss 2.6041131019592285\n",
            "Epoch 900, Loss 2.6011006832122803\n",
            "Epoch 1000, Loss 2.5459964275360107\n"
          ]
        }
      ]
    }
  ]
}